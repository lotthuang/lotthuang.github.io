<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>第十篇 Linux Net子系统总结、收发包流程再次梳理 - Lott's Blog</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "\u7b2c\u5341\u7bc7 Linux Net\u5b50\u7cfb\u7edf\u603b\u7ed3\u3001\u6536\u53d1\u5305\u6d41\u7a0b\u518d\u6b21\u68b3\u7406";
        var mkdocs_page_input_path = "chapter_3/10_linux_net.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Lott's Blog
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">首页</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">第一章 各种杂项基础</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/1_make_blog/">第一篇 利用Github Pages和mkdocs搭建个人博客</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/2_nginx_loadbalancing_strategy/">第二篇 Nginx的均衡策略</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/3_linux_kernel_basic/">第三篇 Linux内核基础</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/4_linux_kernel_mm_basic/">第四篇 Linux内核空间内存概述</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/5_how_kernel_call_bios/">第五篇 Linux内核调用BIOS案例</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/9_linux_arch_overall_introduction/">第九篇 Linux架构整体介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/10_virtual_machine_on_mac_m1_chip/">第十篇 Mac M1芯片电脑利用Qemu安装Debian虚拟机</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/11_nftables/">第十一篇 Debian 11以及往后版本nftables防火墙配置</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/12_ansible_basic_use/">第十二篇 Ansible简单使用笔记</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/13_tty_terminal_console/">第十三篇 终端(Terminal)、TTY、PTY和控制台(Console)区别</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/14_qemu_use/">第十四篇 QEMU模拟各类开发板</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/15_bosun/">第十五篇 Bosun使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/16_assembler/">第十六篇 汇编语言使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/17_makefile/">第十七篇 Makefile使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/19_network_ratelimit/">第十九篇 Linux网络限速</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/20_mac_os_skills/">第二十篇 MacOS使用技巧</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_1/21_debian_skills/">第二十一篇 Debian使用技巧</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第二章 运维工作总结</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/yunweijiazhi/">第一篇 运维的价值和目标</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/yunweizhiyeguifan/">第二篇 业务SRE的职业规范</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/yunweipingtailinian/">第三篇 运维平台建设的理念</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/guzhangguanli/">第四篇 故障管理概述</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/biangengguanli/">第五篇 变更管理概述</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/rongliangguanli/">第六篇 容量管理概述</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/jiagoushisiweihetezhi/">第七篇 架构师思维和特质以及中间件技术梳理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/cicd/">第八篇 CICD流程概述</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/9_changkanchangxin/">第九篇 常看常新</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/12_monitor_and_alert/">第十二篇 监控报警最佳实践</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/13_classic_thinkings/">第十三篇 经典思维</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_2/14_performance_optimization_topic/">第十四篇 性能优化专题</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第三章 Linux Net子系统</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../1_linux_kernel_net/">第一篇 Linux数据包接收路径梳理和常用调优技能</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2_estab_queue_monitoring/">第二篇 Linux下如何观察完整连接队列的情况</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../3_net_ipv4_tcp_syn_retries/">第三篇 理解net.ipv4.tcp_syn_retries设置</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../4_linux_kernel_net_v2/">第四篇 Linux网络数据包的揭秘以及常见的调优方式总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../5_conntrack/">第五篇 conntrack的常见应用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../6_tcp/">第六篇 TCP协议总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../7_udp/">第七篇 UDP协议总结以及TCP和UDP的区别</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../8_io_multiplexing/">第八篇 IO多路复用总结</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">第十篇 Linux Net子系统总结、收发包流程再次梳理</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#_2">一 内核网络层核心架构图</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#_3">(一) 核心数据结构</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#1-struct-softnet_data">1 struct softnet_data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2-struct-net_protocol">2 struct net_protocol</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#21-inet_protosmax_inet_protos">2.1 inet_protos[MAX_INET_PROTOS] 数组</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#22-ptype_base">2.2 ptype_base 哈希表</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#3-struct-net">3 struct net</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#4-struct-net_device">4 struct net_device</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#41-ptype_base-ptype_all">4.1 ptype_base 和 ptype_all</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#42-struct-in_device-ip">4.2 struct in_device （IP 配置块）</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#43-struct-in_ifaddr-ipv4">4.3 struct in_ifaddr （IPv4 地址）</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#5-struct-net_device_ops">5 struct net_device_ops</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#51-struct-ethtool_ops">5.1 struct ethtool_ops</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#6-struct-napi_struct">6 struct napi_struct</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#7-sk_buff">7 sk_buff 结构体</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#8-struct-proto-tcp_prot">8 struct proto tcp_prot</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#81-socket-inet-proto">8.1 关于 socket 和 inet 和 proto 的说明</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#9-tcp-sock">9 tcp sock 对象的发送队列是一个链表</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#10-struct-sock">10 struct sock 讲解</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#socket-sock">socket &amp; sock</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#102-struct-inet_connection_sock">10.2 struct inet_connection_sock</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#103-struct-inet_sock">10.3 struct inet_sock</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#104">10.4 半连接哈希表和全连接队列(半连接队列和全连接队列)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#105-struct-request_sock-mini-sock-to-represent-a-connection-request">10.5 struct request_sock - mini sock to represent a connection request</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#11-linux-softnet_data">11 linux 实现软中断的重要数据结构 softnet_data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#12-struct-msghdr">12 struct msghdr</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#13-struct-iovec">13 struct iovec</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#14">14 等待队列相关结构体</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#15-struct-eventpoll">15 struct eventpoll</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#16-struct-socket_wq">16 struct socket_wq</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#17-struct-neighbour">17 struct neighbour</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#18-struct-inet_hashinfo-tcp_hashinfo">18 struct inet_hashinfo tcp_hashinfo</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#19-dst_entry">19 dst_entry</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_5">(二) 核心函数</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#l3-l4">L3-&gt;L4</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#l2-l3">L2-&gt;L3</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#1-ip_rcv">1 ip_rcv</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2-tcp_v4_rcv">2 tcp_v4_rcv</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#4-udp_rcv">4 udp_rcv</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#5-napi_schedule">5 napi_schedule 系列函数</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#6-dev_hard_start_xmit">6 dev_hard_start_xmit 真正提交给硬件网卡发包函数</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#7-tcp_transmit_skb">7 tcp_transmit_skb</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_6">二 发包路径</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1">1 架构图和总体纲领</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2">2 硬中断代码</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3">3 内核事先做了哪些准备工作</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#4">4 具体路径</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#41">4.1 协议栈</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#42">4.2 邻居子系统</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#43">4.3 网络设备子系统</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#44">4.4 具体驱动程序</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_7">三 收包路径</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1_1">1 架构图和总体纲领</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2_1">2 硬中断代码</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#21">2.1 网卡硬中断流程</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3_1">3 为了实现软中断内核做了哪些准备工作</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#31-ksoftirqd">3.1 开启 ksoftirqd 线程</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#32">3.2 统一实现、统一注册协议处理函数</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#4_1">4 具体路径</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#40">4.0 简化版本:</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#41_1">4.1 硬中断之前和硬中断的工作</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#42_1">4.2 软中断的工作</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_8">四 常见问题&amp;&amp;常用技能</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1-cpu">1 调整网卡软中断到不同 CPU</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#_9">默认情况</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-netfilter-iptables">2 netfilter 和 iptables 过滤是在哪个环节/环境下执行的?</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3-tcpdump">3 tcpdump 是在哪个环节执行的?</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#31">3.1 收包方向</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#32_1">3.2 发包方向</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#4-ebpf-xdp">4 eBPF 和 XDP 的包过滤移交到网卡上具体是啥?</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../11_igb/">第十一篇 igb网卡驱动梳理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../12_rss_rps_rfs_xps/">第十二篇 Linux网卡rss和rps和rfs和xps</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../13_linghunkaowen/">第十三篇 Linux网络子系统灵魂拷问</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第四章 Linux CPU子系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/1_process_ulimit/">第一篇 进程Max open files的设置</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/2_pid_max/">第二篇 kernel源码中pid的最大值</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/3_interrupt_one/">第三篇 硬中断</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/5_soft_irq/">第四篇 软中断</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/4_process_thread_coroutine/">第五篇 进程、线程、协程</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/6_cpu_famous_register/">第六篇 CPU中的著名寄存器梳理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/7_linux_signal/">第七篇 Linux信号机制</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/8_linux_scheduler/">第八篇 Linux进程调度</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/10_linux_context_switch/">第十篇 低视角看context switch</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/11_linux_cpu_usage/">第十一篇 CPU利用率统计</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/12_linux_preemption/">第十二篇 内核抢占基础篇</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/13_interrupt_handler_examples/">第十三篇 中断处理程序的例子</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/14_linux_kernel_tracing/">第十四篇 理解内核追踪机制</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/15_task_struct/">第十五篇 task_struct结构体总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/16_init_call/">第十六篇 Linux各类initcall总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_4/17_coredump/">第十七篇 Linux coredump总结</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第五章 Linux IO子系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/1_four_object_of_VFS/">第一篇 VFS四大对象inode、dentry、super_block、file数据结构总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/2_io_stack/">第二篇 Linux IO栈总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/3_zero_copy/">第三篇 Linux 零拷贝技术总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/4_io_scheduler_layer/">第四篇 Block IO Layer(块IO层)总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/6_fss/">第六篇 各类文件系统梳理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/7_three_object_of_block_device/">第七篇 块设备三大对象block_device、gendisk、hd_struct数据结构总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/8_kobjet_kset_ktype/">第八篇 Linux设备模型的基石:kset/kobject/ktype</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_5/9_io_uring/">第八篇 io_uring研究和总结</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第六章 Linux Mem子系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/1_linux_buddy/">第一篇 伙伴系统介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/2_slab/">第二篇 Slab层介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/3_virtual_memory/">第三篇 进程虚拟地址空间介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/4_linux_mem_theory/">第四篇 Linux内存管理基础篇</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/6_alloc_mem_apis/">第六篇 Linux内核内存分配API函数分类</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/7_physical_memory/">第七篇 关于物理内存管理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/8_linux_mm_update/">第八篇 Linux内存管理升级篇</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_6/9_page_cache_and_buffer_cache/">第九篇 PageCache和BufferCache演进过程</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第七章 运维小技巧和工具汇总和积累</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/1_sync_file/">第一篇 不同主机之间的文件传输</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/2_sre_tools_python/">第二篇 运维生产力工具(python篇)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/3_sre_tools_website/">第三篇 运维生产力工具(网站工具)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/4_sre_tools_shell/">第四篇 运维生产力工具(shell篇)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/5_server_side_evolution/">第五篇 服务端进化过程梳理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/7_charset/">第七篇 字符集和编码</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/8_locale/">第八篇 locale设置</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/9_http_protocol/">第九篇 http协议</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_7/10_http_cookie_session_token/">第十篇 cookie和session和token总结</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第八章 职场通用经验</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/1_xuexixinde/">第一篇 学习心得和工作箴言</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/2_bangzhuqitatongxue/">第二篇 工作能力评价体系</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/3_yewuliuchengshuyu/">第三篇 项目管理总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/4_jishunenglimoxing/">第四篇 技术能力模型总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/5_chanpingsiwei_1/">第五篇 产品思维总结和梳理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/6_jixiaobaogao/">第六篇 如何写绩效报告</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/7_xiangshangguanli/">第七篇 如何向上管理</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/9_speaking_skills/">第九篇 沟通能力和演讲技巧</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/10_jixiaofangtan/">第十篇 绩效访谈总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_8/11_classic_technical_thinking/">第十一篇 经典技术思维汇总</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第九章 编程经验和各类开源软件</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/1_suanfa/">第一篇 算法总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/2_data_structure/">第二篇 数据结构总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/3_c_lang/">第三篇 c语言总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/4_symbol_table/">第四篇 符号表总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/5_golang/">第五篇 go语言总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/6_linux_kernel_patch/">第六篇 手把手教你向Linux Kernel提交Patch</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/7_python/">第七篇 python语言总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/8_rsyslog/">第八篇 rsyslog</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/9_falcon/">第九篇 falcon和夜莺</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/10_json_schema/">第十篇 JSON-Schema使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/11_prometheus/">第十一篇 Prometheus使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/12_grafana/">第十二篇 Grafana使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/13_mysql/">第十三篇 MySQL使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/14_mongodb/">第十四篇 MongoDB使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/15_supervisor/">第十五篇 Supervisor使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/16_redis/">第十六篇 Redis使用</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/17_mongodb_2/">第十七篇 MongoDB基础</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/18_elk_2/">第十八篇 ELK基础</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/19_gitlab_ce/">第十九篇 Gitlab-CE维护</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/20_minio/">第二十篇 MinIO维护</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/21_elk_1/">第二十一篇 ELK维护</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_9/22_httpdns/">第二十二篇 HTTPDNS梳理</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第十章 容器化和Kubernetes</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/1_dockerfile_best_practice/">第一篇 dockerfile最佳实践</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/2_benefits_of_containerization/">第二篇 容器化好处</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/3_docker_basic_concepts/">第三篇 容器化基础概念</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/4_kubernetes_basic_concepts/">第四篇 Kubernetes基础概念</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/6_production_kubernetes_best_practice/">第六篇 生产环境Kubernetes部署最佳实践</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/7_production_kerbernetes_yaml/">第七篇 生产环境Kubernetes YAML模版</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/8_caclio/">第八篇 Kubernetes网络插件之Caclio</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/9_cloudnative_monitor/">第九篇 云原生监控</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/10_event/">第十篇 彻底搞懂Kubernetes Event</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/11_best_practices/">第十一篇 Kubernetes最佳实践</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/12_cloud_network/">第十二篇 云网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_10/13_containerd/">第十三篇 Containerd</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第十一章 SRE稳定性建设从理论到实践</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_11/1_theory/">第一篇 稳定性概论</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_11/10_high_avaliable/">第二篇 高可用原则总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_11/2_slo_process/">第三篇 SLO实施流程</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_11/3_stress/">第四篇 压测经验总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_11/11_stability/">第五篇 稳定性理论总结</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第十二章 商业化知识总结</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_12/1_shifouzhide/">第一篇 如何判断一件事情值不值得做</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第十三章 芯片知识总结</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_13/1_about_riscv/">第一篇 RISC-V 简介</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第十四章 Linux驱动方向总结</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/1_demo_of_led_driver/">第一篇 驱动程序的Hello World</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/2_qianrushidev/">第二篇 嵌入式开发基础</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/3_linux_kernel_api_list/">第三篇 Linux内核API汇总和各类风格总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/4_zifushebeiqudongyuanli/">第四篇 字符设备驱动实现原理和物理内存地址空间和IO端口空间</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/5_device_tree/">第五篇 platform总线模型和设备树</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/6_device_driver_sop/">第六篇 从阅读硬件手册开始设备驱动程序编写</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/7_linux_kernel_clipping/">第七篇 Linux移植</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/8_linux_driver_framework/">第八篇 Linux驱动框架和SoC驱动框架</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/9_linux_device_class/">第九篇 Linux设备分类</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/10_u_boot/">第十篇 u-boot梳理和总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/11_linux_reset_subsystem_and_framework/">第十一篇 Linux Reset子系统及其框架</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/12_pinctl_and_gpio/">第十二篇 Linux pinctl和gpio子系统及其框架</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/13_embedded_development_direction/">第十三篇 嵌入式开发方向和分类</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/14_linux_kernel_trimming/">第十四篇 Linux内核裁剪总结</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_14/15_imx6ull/">第十五篇 IMX6UL实验记录</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第十五章 AI+嵌入式结合方向汇总</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_15/1_demo_of_ai/">第一篇 AI程序 Hello World</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">第十六章 网络安全方向</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../chapter_16/1_network_attack/">第一篇 认识各种网络攻击</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Lott's Blog</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">第三章 Linux Net子系统</li>
      <li class="breadcrumb-item active">第十篇 Linux Net子系统总结、收发包流程再次梳理</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="_1">概述</h1>
<ul>
<li>本篇总结 Linux Net 子系统的<strong>核心架构图、核心数据结构</strong></li>
<li><strong>发包路径</strong>和<strong>收包路径</strong>, 每个路径上怎么监控和调优</li>
<li>以及<strong>一些常见的问题</strong></li>
</ul>
<h2 id="_2">一 内核网络层核心架构图</h2>
<ul>
<li>这里总结一下内核的核心实现, 方便快速掌握其核心原理</li>
</ul>
<p><img alt="png" src="../../img/7467AD50-A529-4B0E-88CB-0B685860CFEE.png" /></p>
<ul>
<li>
<ul>
<li>其中<code>dev.c</code> 是指 <code>//file: net/core/dev.c</code>, 著名的<code>napi_gro_receive</code>函数和<code>netif_receive_skb</code>函数就是在这里实现的</li>
</ul>
</li>
</ul>
<h3 id="_3">(一) 核心数据结构</h3>
<h4 id="1-struct-softnet_data">1 struct softnet_data</h4>
<ul>
<li>linux 内核通过调用 subsys_initcall 来初始化各个子系统，在源代码目录里你可以 grep 出许多对这个函数的调用。这里我们要说的是网络子系统的初始化，会执行到 net_dev_init 函数。</li>
</ul>
<pre><code>//file: net/core/dev.c

static int __init net_dev_init(void){

    ......

    for_each_possible_cpu(i) {
        struct softnet_data *sd = &amp;per_cpu(softnet_data, i);

        memset(sd, 0, sizeof(*sd));
        skb_queue_head_init(&amp;sd-&gt;input_pkt_queue);
        skb_queue_head_init(&amp;sd-&gt;process_queue);
        sd-&gt;completion_queue = NULL;
        INIT_LIST_HEAD(&amp;sd-&gt;poll_list); /*将poll_list成员（该成员是一个struct list_head）的前驱和后置都指向自己*/
        ......
    }
    ......
    open_softirq(NET_TX_SOFTIRQ, net_tx_action);
    open_softirq(NET_RX_SOFTIRQ, net_rx_action);

}

subsys_initcall(net_dev_init);

</code></pre>
<p>在这个函数里，会为<strong>每个 CPU 都申请一个 softnet_data 数据结构</strong>，在这个数据结构里的<code>poll_list</code>是等待驱动程序将其 poll 函数注册进来，稍后网卡驱动初始化的时候我们可以看到这一过程。</p>
<p>核心成员:</p>
<ul>
<li>poll_list.</li>
<li>
<ul>
<li>是什么原理呢, 这里就是一个 poll 函数链表. 每次网卡有数据来时, 硬中断把自己的 poll 函数放到这个链表里来.</li>
</ul>
</li>
<li>
<ul>
<li>是一个链表，后续将 struct napi_struct 加入 softnet_data 结构中 poll_list 链表里来，就可以通过 softnet_data 的 poll_list 链表遍历所有的 napi_struct 实例，每个 napi_struct 实例其实包含了特定于<strong>网卡</strong>的 poll 函数，后续调用 napi_struct 的 poll 函数其实就是调用<strong>网卡</strong>的 poll 函数.</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li>并且这里说的<strong>网卡</strong>, 比如是 igb 网卡，那么其实指的是 struct igb_q_vector 结构，该结构表示 igb 网卡的一个队列,每个 igb 网卡可以有 8 个队列，每个队列对应一个真实的硬中断号.</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);
也就是说待会儿内核跑软中断时,要执行我的poll函数(这里我就是一张网卡)
</code></pre>
<ul>
<li>软中断阶段:</li>
</ul>
<pre><code>
static void net_rx_action(struct softirq_action *h){
    struct softnet_data *sd = &amp;__get_cpu_var(softnet_data);
    unsigned long time_limit = jiffies + 2;
    int budget = netdev_budget;
    void *have;

    local_irq_disable();
    while (!list_empty(&amp;sd-&gt;poll_list)) {
        ......
        n = list_first_entry(&amp;sd-&gt;poll_list, struct napi_struct, poll_list);

        work = 0;
        if (test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)) {
            work = n-&gt;poll(n, weight);
            trace_napi_poll(n);
        }
        budget -= work;
    }

}

简单理解就是在跑特定于网卡的poll函数.
</code></pre>
<ul>
<li><a href="http://www.hyuuhit.com/2018/07/25/receive-packet/">softnet_data</a></li>
</ul>
<h4 id="2-struct-net_protocol">2 struct net_protocol</h4>
<ul>
<li>代表一个传输层协议，比如 tcp、udp</li>
</ul>
<pre><code>static const struct net_protocol tcp_protocol = {
    .early_demux    =   tcp_v4_early_demux,
    .handler    =   tcp_v4_rcv,
    .err_handler    =   tcp_v4_err,
    .no_policy  =   1,
    .netns_ok   =   1,

};

static const struct net_protocol udp_protocol = {
    .handler =  udp_rcv,
    .err_handler =  udp_err,
    .no_policy =    1,
    .netns_ok = 1
};


</code></pre>
<h4 id="21-inet_protosmax_inet_protos">2.1 inet_protos[MAX_INET_PROTOS] 数组</h4>
<ul>
<li>
<p><strong>inet_protos</strong>是一个全局的指针数组, 存有所有<strong>net_protocol</strong></p>
</li>
<li>
<p><code>const struct net_protocol __rcu *inet_protos[MAX_INET_PROTOS] __read_mostly;</code></p>
</li>
<li>
<ul>
<li>其中 MAX_INET_PROTOS 为 256</li>
</ul>
</li>
<li>
<ul>
<li><code>include/net/protocol.h:38:#define MAX_INET_PROTOS 256</code></li>
</ul>
</li>
</ul>
<p><img alt="png" src="../../img/0E1DFAF8-8B7A-4040-BE24-1E6D82897CBC.jpg" /></p>
<h4 id="22-ptype_base">2.2 ptype_base 哈希表</h4>
<ul>
<li><code>net/core/dev.c:146:struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;</code></li>
<li><code>#define PTYPE_HASH_SIZE (16)</code></li>
</ul>
<h4 id="3-struct-net">3 struct net</h4>
<ul>
<li>网络命名空间</li>
</ul>
<p>struct net 结构体表示的内核中的网络命名空间(net_namespace)。在 linux 内核中，每一个网络设备(struct net_device)都有一个所属的网络命名空间</p>
<p>网络系统在初始化的时候会初始化一个初始网络命名空间，即 init_net 命名空间。后续创建的 net namespace 命名空间会和 init_net 一起通过 list 项组织起来，且每个网络设备都对应一个命名空间，同一命名空间下的网络设备通过 dev_base_head 组织在一起。组织结构如下:</p>
<p><img alt="png" src="../../img/12FD092E-94F4-444F-98CA-44D0EE9F219A.jpeg" /></p>
<p>struct net 内的一些重要成员：</p>
<ul>
<li><code>struct net_device *loopback_dev;</code></li>
</ul>
<h4 id="4-struct-net_device">4 struct net_device</h4>
<ul>
<li>内核层面描述一张网卡</li>
<li>
<p>每个 struct net, 都至少有一个环回接口 lo, 该接口就是用 struct net_device 表示</p>
</li>
<li>
<p><a href="https://blog.csdn.net/viewsky11/article/details/53046787">在 linux 中使用 struct net_device 结构体来描述每一个网络设备。同时这个用来刻画网络设备的 struct net_device 结构体包含的字段非常的多，以至于内核的开发者都觉得在现在的 linux 内核中，这个 struct net_device 是一个大的错误。</a></p>
</li>
<li>
<ul>
<li>并且注册 net_device(<code>register_netdev(struct net_device *dev)</code>)是注册到一个链表中</li>
</ul>
</li>
<li>
<p><a href="https://www.oreilly.com/library/view/linux-device-drivers/0596000081/ch14s03.html"><strong>struct net_device</strong> can be conceptually divided into two parts: visible and invisible.</a> net_device 结构体(有众多成员，总体来说成员)可以分为两个部分： 可见和不可见.</p>
</li>
<li>
<p>内核使用 net_device 表示网卡。网卡可以分为<strong>物理网卡</strong>和<strong>虚拟网卡</strong>。物理网卡是指真正能把报文发出本机的网卡，包括真实物理机的网卡以及 VM 虚拟机的网卡，而像 tun/tap，vxlan、veth pair 这样的则属于虚拟网卡的范畴。</p>
</li>
<li>
<p>如下图所示，<strong>每个网卡都有两端</strong>，一端是协议栈(IP、TCP、UDP)，另一端则有所区别，对物理网卡来说，这一端是网卡生产厂商提供的设备驱动程序，而对虚拟网卡来说差别就大了，正是由于虚拟网卡的存在，内核才能支持各种隧道封装、容器通信等功能。</p>
</li>
</ul>
<p><img alt="png" src="../../img/WechatIMG535.jpg" /></p>
<ul>
<li>net_device 源码如下:</li>
</ul>
<pre><code>
struct net_device {

    /*
     * This is the first field of the &quot;visible&quot; part of this structure
     * (i.e. as seen by users in the &quot;Space.c&quot; file).  It is the name
     * of the interface.
     */
    char            name[IFNAMSIZ];

    /* device name hash chain, please keep it close to name[] */
    struct hlist_node   name_hlist;

    /* snmp alias */
    char            *ifalias;

    /*
     *  I/O specific fields
     *  FIXME: Merge these and struct ifmap into one
     */
    unsigned long       mem_end;    /* shared mem end   */
    unsigned long       mem_start;  /* shared mem start */
    unsigned long       base_addr;  /* device I/O address   */
    int         irq;        /* device IRQ number    */

    /*
     *  Some hardware also needs these fields, but they are not
     *  part of the usual set specified in Space.c.
     */

    unsigned long       state;

    struct list_head    dev_list;
    struct list_head    napi_list;
    struct list_head    unreg_list;
    struct list_head    close_list;

    /* directly linked devices, like slaves for bonding */
    struct {
        struct list_head upper;
        struct list_head lower;
    } adj_list;

    /* all linked devices, *including* neighbours */
    struct {
        struct list_head upper;
        struct list_head lower;
    } all_adj_list;


    /* currently active device features */
    netdev_features_t   features;
    /* user-changeable features */
    netdev_features_t   hw_features;
    /* user-requested features */
    netdev_features_t   wanted_features;
    /* mask of features inheritable by VLAN devices */
    netdev_features_t   vlan_features;
    /* mask of features inherited by encapsulating devices
     * This field indicates what encapsulation offloads
     * the hardware is capable of doing, and drivers will
     * need to set them appropriately.
     */
    netdev_features_t   hw_enc_features;
    /* mask of fetures inheritable by MPLS */
    netdev_features_t   mpls_features;

    /* Interface index. Unique device identifier    */
    int         ifindex;
    int         iflink;

    struct net_device_stats stats;

    /* dropped packets by core network, Do not use this in drivers */
    atomic_long_t       rx_dropped;
    atomic_long_t       tx_dropped;

    /* Stats to monitor carrier on&lt;-&gt;off transitions */
    atomic_t        carrier_changes;

#ifdef CONFIG_WIRELESS_EXT
    /* List of functions to handle Wireless Extensions (instead of ioctl).
     * See &lt;net/iw_handler.h&gt; for details. Jean II */
    const struct iw_handler_def *   wireless_handlers;
    /* Instance data managed by the core of Wireless Extensions. */
    struct iw_public_data * wireless_data;
#endif
    /* Management operations */
    const struct net_device_ops *netdev_ops;
    const struct ethtool_ops *ethtool_ops;
    const struct forwarding_accel_ops *fwd_ops;

    /* Hardware header description */
    const struct header_ops *header_ops;

    unsigned int        flags;  /* interface flags (a la BSD)   */
    unsigned int        priv_flags; /* Like 'flags' but invisible to userspace.
                         * See if.h for definitions. */
    unsigned short      gflags;
    unsigned short      padded; /* How much padding added by alloc_netdev() */

    unsigned char       operstate; /* RFC2863 operstate */
    unsigned char       link_mode; /* mapping policy to operstate */

    unsigned char       if_port;    /* Selectable AUI, TP,..*/
    unsigned char       dma;        /* DMA channel      */

    unsigned int        mtu;    /* interface MTU value      */
    unsigned short      type;   /* interface hardware type  */
    unsigned short      hard_header_len;    /* hardware hdr length  */

    /* extra head- and tailroom the hardware may need, but not in all cases
     * can this be guaranteed, especially tailroom. Some cases also use
     * LL_MAX_HEADER instead to allocate the skb.
     */
    unsigned short      needed_headroom;
    unsigned short      needed_tailroom;

    /* Interface address info. */
    unsigned char       perm_addr[MAX_ADDR_LEN]; /* permanent hw address */
    unsigned char       addr_assign_type; /* hw address assignment type */
    unsigned char       addr_len;   /* hardware address length  */
    unsigned short      neigh_priv_len;
    unsigned short          dev_id;     /* Used to differentiate devices
                         * that share the same link
                         * layer address
                         */
    unsigned short          dev_port;   /* Used to differentiate
                         * devices that share the same
                         * function
                         */
    spinlock_t      addr_list_lock;
    struct netdev_hw_addr_list  uc; /* Unicast mac addresses */
    struct netdev_hw_addr_list  mc; /* Multicast mac addresses */
    struct netdev_hw_addr_list  dev_addrs; /* list of device
                            * hw addresses
                            */
#ifdef CONFIG_SYSFS
    struct kset     *queues_kset;
#endif

    bool            uc_promisc;
    unsigned int        promiscuity;
    unsigned int        allmulti;


    /* Protocol specific pointers */

#if IS_ENABLED(CONFIG_VLAN_8021Q)
    struct vlan_info __rcu  *vlan_info; /* VLAN info */
#endif
#if IS_ENABLED(CONFIG_NET_DSA)
    struct dsa_switch_tree  *dsa_ptr;   /* dsa specific data */
#endif
#if IS_ENABLED(CONFIG_TIPC)
    struct tipc_bearer __rcu *tipc_ptr; /* TIPC specific data */
#endif
    void            *atalk_ptr; /* AppleTalk link   */
    struct in_device __rcu  *ip_ptr;    /* IPv4 specific data   在这个结构体中存放ipv4地址*/
    struct dn_dev __rcu     *dn_ptr;        /* DECnet specific data */
    struct inet6_dev __rcu  *ip6_ptr;       /* IPv6 specific data */
    void            *ax25_ptr;  /* AX.25 specific data */
    struct wireless_dev *ieee80211_ptr; /* IEEE 802.11 specific data,
                           assign before registering */

/*
 * Cache lines mostly used on receive path (including eth_type_trans())
 */
    unsigned long       last_rx;    /* Time of last Rx */

    /* Interface address info used in eth_type_trans() */
    unsigned char       *dev_addr;  /* hw address, (before bcast
                           because most packets are
                           unicast) */


#ifdef CONFIG_SYSFS
    struct netdev_rx_queue  *_rx;

    /* Number of RX queues allocated at register_netdev() time */
    unsigned int        num_rx_queues;

    /* Number of RX queues currently active in device */
    unsigned int        real_num_rx_queues;

#endif

    rx_handler_func_t __rcu *rx_handler;
    void __rcu      *rx_handler_data;

    struct netdev_queue __rcu *ingress_queue;
    unsigned char       broadcast[MAX_ADDR_LEN];    /* hw bcast add */


/*
 * Cache lines mostly used on transmit path
 */
    struct netdev_queue *_tx ____cacheline_aligned_in_smp;

    /* Number of TX queues allocated at alloc_netdev_mq() time  */
    unsigned int        num_tx_queues;

    /* Number of TX queues currently active in device  */
    unsigned int        real_num_tx_queues;

    /* root qdisc from userspace point of view */
    struct Qdisc        *qdisc;

    unsigned long       tx_queue_len;   /* Max frames per queue allowed */
    spinlock_t      tx_global_lock;

#ifdef CONFIG_XPS
    struct xps_dev_maps __rcu *xps_maps;
#endif
#ifdef CONFIG_RFS_ACCEL
    /* CPU reverse-mapping for RX completion interrupts, indexed
     * by RX queue number.  Assigned by driver.  This must only be
     * set if the ndo_rx_flow_steer operation is defined. */
    struct cpu_rmap     *rx_cpu_rmap;
#endif

    /* These may be needed for future network-power-down code. */

    /*
     * trans_start here is expensive for high speed devices on SMP,
     * please use netdev_queue-&gt;trans_start instead.
     */
    unsigned long       trans_start;    /* Time (in jiffies) of last Tx */

    int         watchdog_timeo; /* used by dev_watchdog() */
    struct timer_list   watchdog_timer;

    /* Number of references to this device */
    int __percpu        *pcpu_refcnt;

    /* delayed register/unregister */
    struct list_head    todo_list;
    /* device index hash chain */
    struct hlist_node   index_hlist;

    struct list_head    link_watch_list;

    /* register/unregister state machine */
    enum { NETREG_UNINITIALIZED=0,
           NETREG_REGISTERED,   /* completed register_netdevice */
           NETREG_UNREGISTERING,    /* called unregister_netdevice */
           NETREG_UNREGISTERED, /* completed unregister todo */
           NETREG_RELEASED,     /* called free_netdev */
           NETREG_DUMMY,        /* dummy device for NAPI poll */
    } reg_state:8;

    bool dismantle; /* device is going do be freed */

    enum {
        RTNL_LINK_INITIALIZED,
        RTNL_LINK_INITIALIZING,
    } rtnl_link_state:16;

    /* Called from unregister, can be used to call free_netdev */
    void (*destructor)(struct net_device *dev);

#ifdef CONFIG_NETPOLL
    struct netpoll_info __rcu   *npinfo;
#endif

#ifdef CONFIG_NET_NS
    /* Network namespace this network device is inside */
    struct net      *nd_net;
#endif

    /* mid-layer private */
    union {
        void                *ml_priv;
        struct pcpu_lstats __percpu *lstats; /* loopback stats */
        struct pcpu_sw_netstats __percpu    *tstats;
        struct pcpu_dstats __percpu *dstats; /* dummy stats */
        struct pcpu_vstats __percpu *vstats; /* veth stats */
    };
    /* GARP */
    struct garp_port __rcu  *garp_port;
    /* MRP */
    struct mrp_port __rcu   *mrp_port;

    /* class/net/name entry */
    struct device       dev;
    /* space for optional device, statistics, and wireless sysfs groups */
    const struct attribute_group *sysfs_groups[4];
    /* space for optional per-rx queue attributes */
    const struct attribute_group *sysfs_rx_queue_group;

    /* rtnetlink link ops */
    const struct rtnl_link_ops *rtnl_link_ops;

    /* for setting kernel sock attribute on TCP connection setup */
#define GSO_MAX_SIZE        65536
    unsigned int        gso_max_size;
#define GSO_MAX_SEGS        65535
    u16         gso_max_segs;

#ifdef CONFIG_DCB
    /* Data Center Bridging netlink ops */
    const struct dcbnl_rtnl_ops *dcbnl_ops;
#endif
    u8 num_tc;
    struct netdev_tc_txq tc_to_txq[TC_MAX_QUEUE];
    u8 prio_tc_map[TC_BITMASK + 1];

#if IS_ENABLED(CONFIG_FCOE)
    /* max exchange id for FCoE LRO by ddp */
    unsigned int        fcoe_ddp_xid;
#endif
#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
    struct netprio_map __rcu *priomap;
#endif
    /* phy device may attach itself for hardware timestamping */
    struct phy_device *phydev;

    struct lock_class_key *qdisc_tx_busylock;

    /* group the device belongs to */
    int group;

    struct pm_qos_request   pm_qos_req;
};

</code></pre>
<h4 id="41-ptype_base-ptype_all">4.1 ptype_base 和 ptype_all</h4>
<ul>
<li>ptype_base 为一个 hash 表,而 ptype_all 为一个双向链表.每一个里面注册的协议都用一个 struct packet_type 表示. packet_type 数据结构包含协议类型、指向网络设备的指针、指向协议的接收数据处理例程的指针, 如下：</li>
</ul>
<pre><code>struct packet_type
{
    unsigned short        type;    /*协议类型*/
    struct net_device     *dev;
    int            (*func) (struct sk_buff *, struct net_device *,
                     struct packet_type *);
    void            *data;    /* Private to the packet type        */
    struct packet_type    *next;
};

</code></pre>
<ul>
<li>
<p>func 是该结构体主要成员，指向网络层的一个函数。如果分组的类型适当，就将其传给对应的处理函数，其中 ip 包的处理函数就是<code>ip_rcv</code>.</p>
</li>
<li>
<p><code>netif_receive_skb()</code>函数，遍历<code>ptype_all</code>链表，找到合适的<code>packet_type</code>, 然后调用其<code>func</code>指针指向的处理函数(比如<code>ip_rcv()</code>)</p>
</li>
</ul>
<h4 id="42-struct-in_device-ip">4.2 struct in_device （IP 配置块）</h4>
<p><img alt="png" src="../../img/WechatIMG533.jpg" /></p>
<pre><code>
struct in_device
{
    struct net_device *dev; // 回指网络设备
    atomic_t refcnt;
    // 当一个IP配置块将要被销毁时，先设置该标记为1，当引用计数为0时再执行内存回收
    int dead;
    // 每个网络设备可以配置多个IP地址，所以是列表
    struct in_ifaddr *ifa_list; /* IP ifaddr chain */
    // 多播相关的结构
    rwlock_t mc_list_lock;
    struct ip_mc_list   *mc_list;   /* IP multicast filter chain    */
    spinlock_t      mc_tomb_lock;
    struct ip_mc_list   *mc_tomb;
    unsigned long       mr_v1_seen;
    unsigned long       mr_v2_seen;
    unsigned long       mr_maxdelay;
    unsigned char       mr_qrv;
    unsigned char       mr_gq_running;
    unsigned char       mr_ifc_count;
    struct timer_list   mr_gq_timer;    /* general query timer */
    struct timer_list   mr_ifc_timer;   /* interface change timer */

    // 网络设备定义的邻居子系统配置参数
    struct neigh_parms  *arp_parms;
    // 影响该网络设备的IP系统配置
    struct ipv4_devconf cnf;
    // 使用RCU机制回收IP配置块
    struct rcu_head rcu_head;
};
</code></pre>
<h4 id="43-struct-in_ifaddr-ipv4">4.3 struct in_ifaddr （IPv4 地址）</h4>
<pre><code>
struct in_ifaddr
{
    struct in_ifaddr *ifa_next; // 同一个网络设备上配置的IP地址组成成链表
    struct in_device *ifa_dev; // 回指网络设备
    struct rcu_head rcu_head; // 使用RCU保护本IP地址的修改
    // 对于支持广播的网络，ifa_local和ifa_address一样保存的都是本端地址；
    // 对于点对点网络，ifa_address保存的是对端IP地址
    __be32  ifa_local;
    __be32  ifa_address;
    __be32  ifa_mask; // 子网掩码
    __be32  ifa_broadcast; // 广播地址
    __be32  ifa_anycast;
    unsigned char ifa_scope; // IP地址作用域，见下文
    unsigned char ifa_flags;
    unsigned char ifa_prefixlen; // 子网掩码长度
    char ifa_label[IFNAMSIZ]; // 网络地址别名
};
</code></pre>
<ul>
<li>参考<a href="https://blog.csdn.net/wangquan1992/article/details/109187964">linux 内核协议栈 IP 地址数据结构</a></li>
</ul>
<h4 id="5-struct-net_device_ops">5 struct net_device_ops</h4>
<ul>
<li>每个 net_device 具备的一些能力, 比如 open,read,write,poll,ioctl 等</li>
<li>net_device_ops 核心描述符内包含了驱动程序填充的各类函数, 包括发送、接收数据包</li>
</ul>
<p>以 <code>net/ethernet/intel/igb/igb_main.c</code>(igb 网卡驱动) 为例子, 包含了<code>open</code>,<code>close</code>,<code>xmit</code>,<code>ioctl</code> 等各种实现</p>
<pre><code>
/file: drivers/net/ethernet/intel/igb/igb_main.c


static const struct net_device_ops igb_netdev_ops = {
    .ndo_open       = igb_open,
    .ndo_stop       = igb_close,
    .ndo_start_xmit     = igb_xmit_frame,
    .ndo_get_stats64    = igb_get_stats64,
    .ndo_set_rx_mode    = igb_set_rx_mode,
    .ndo_set_mac_address    = igb_set_mac,
    .ndo_change_mtu     = igb_change_mtu,
    .ndo_do_ioctl       = igb_ioctl,
    .ndo_tx_timeout     = igb_tx_timeout,
    .ndo_validate_addr  = eth_validate_addr,
    .ndo_vlan_rx_add_vid    = igb_vlan_rx_add_vid,
    .ndo_vlan_rx_kill_vid   = igb_vlan_rx_kill_vid,
    .ndo_set_vf_mac     = igb_ndo_set_vf_mac,
    .ndo_set_vf_vlan    = igb_ndo_set_vf_vlan,
    .ndo_set_vf_rate    = igb_ndo_set_vf_bw,
    .ndo_set_vf_spoofchk    = igb_ndo_set_vf_spoofchk,
    .ndo_get_vf_config  = igb_ndo_get_vf_config,
#ifdef CONFIG_NET_POLL_CONTROLLER
    .ndo_poll_controller    = igb_netpoll,
#endif
    .ndo_fix_features   = igb_fix_features,
    .ndo_set_features   = igb_set_features,
};

</code></pre>
<h4 id="51-struct-ethtool_ops">5.1 struct ethtool_ops</h4>
<p>以<code>net/ethernet/intel/igb/igb_ethool.c</code>为例，这里注册了 igb 网卡对 ethool 的支持.</p>
<pre><code>
static const struct ethtool_ops igb_ethtool_ops = {
    .get_settings       = igb_get_settings,
    .set_settings       = igb_set_settings,
    .get_drvinfo        = igb_get_drvinfo,
    .get_regs_len       = igb_get_regs_len,
    .get_regs       = igb_get_regs,
    .get_wol        = igb_get_wol,
    .set_wol        = igb_set_wol,
    .get_msglevel       = igb_get_msglevel,
    .set_msglevel       = igb_set_msglevel,
    .nway_reset     = igb_nway_reset,
    .get_link       = igb_get_link,
    .get_eeprom_len     = igb_get_eeprom_len,
    .get_eeprom     = igb_get_eeprom,
    .set_eeprom     = igb_set_eeprom,
    .get_ringparam      = igb_get_ringparam,
    .set_ringparam      = igb_set_ringparam,
    .get_pauseparam     = igb_get_pauseparam,
    .set_pauseparam     = igb_set_pauseparam,
    .self_test      = igb_diag_test,
    .get_strings        = igb_get_strings,
    .set_phys_id        = igb_set_phys_id,
    .get_sset_count     = igb_get_sset_count,
    .get_ethtool_stats  = igb_get_ethtool_stats,
    .get_coalesce       = igb_get_coalesce,
    .set_coalesce       = igb_set_coalesce,
    .get_ts_info        = igb_get_ts_info,
    .get_rxnfc      = igb_get_rxnfc,
    .set_rxnfc      = igb_set_rxnfc,
    .get_eee        = igb_get_eee,
    .set_eee        = igb_set_eee,
    .get_module_info    = igb_get_module_info,
    .get_module_eeprom  = igb_get_module_eeprom,
    .get_rxfh_indir_size    = igb_get_rxfh_indir_size,
    .get_rxfh       = igb_get_rxfh,
    .set_rxfh       = igb_set_rxfh,
    .get_channels       = igb_get_channels,
    .set_channels       = igb_set_channels,
    .begin          = igb_ethtool_begin,
    .complete       = igb_ethtool_complete,
};

void igb_set_ethtool_ops(struct net_device *netdev)
{
    netdev-&gt;ethtool_ops = &amp;igb_ethtool_ops;
}

</code></pre>
<h4 id="6-struct-napi_struct">6 struct napi_struct</h4>
<pre><code>
struct napi_struct {
    /* The poll_list must only be managed by the entity which
     * changes the state of the NAPI_STATE_SCHED bit.  This means
     * whoever atomically sets that bit can add this napi_struct
     * to the per-cpu poll_list, and whoever clears that bit
     * can remove from the list right before clearing the bit.
     */
    struct list_head    poll_list;

    unsigned long       state; // state 可以是 NAPI_STATE_SCHED 或 NAPI_STATE_DISABLE，前者表示设备将在内核的下一次循环时被轮询，后者表示轮询己经结束且没有更多的分组等待处理，但设备尚未从轮询表移除。
    int         weight;
    unsigned int        gro_count;
    int         (*poll)(struct napi_struct *, int); //核心成员, 特定于硬件的poll函数
#ifdef CONFIG_NETPOLL
    spinlock_t      poll_lock;
    int         poll_owner;
#endif
    struct net_device   *dev;
    struct sk_buff      *gro_list;
    struct sk_buff      *skb;
    struct list_head    dev_list;
    struct hlist_node   napi_hash_node;
    unsigned int        napi_id;
};

</code></pre>
<p><code>struct napi_struct</code>该结构用于管理轮询表上的设备。其定义如下:</p>
<pre><code>&lt;netdevice.h&gt;
struct napi_struct {
    struct list_head poll_list;
    unsigned long state;
    int weight;
    int (*poll) (struct napi_struct *, int)
};
</code></pre>
<ul>
<li>
<p>轮询表通过一个标准的内核双链表实现，poll_list 用作链表元素。weight 和 poll 的语义同上文 所述。state 可以是 NAPI_STATE_SCHED 或 NAPI_STATE_DISABLE，前者表示设备将在内核的下一次循环时被轮询，后者表示轮询己经结束且没有更多的分组等待处理，但设备尚未从轮询表移除。 请注意，<code>struct napi_struct</code> 经常嵌入到 一个更大的结构中，后者包含了与网卡有关的、特定 手驱 动 稈 序 的 数 据 。 这 样 在 内 核 使 用 poll 函数 轮 询 网 卡 时 ， 可 用 container_of 机 制 获 得 相 关 信 息 。</p>
</li>
<li>
<p>实现 poll 函数需要两个參数:一个指向 napi_struct 实例的指针和一个指定了“预算” 的整数，预算 表示内核允许驱动程序处理的分组数目。</p>
</li>
</ul>
<h4 id="7-sk_buff">7 sk_buff 结构体</h4>
<ul>
<li>
<p>sk_buff 是 Linux 网络中最核心的结构体，它用来管理和控制接收或发送数据包的信息。
  各层协议都依赖于 sk_buff 而存在。</p>
</li>
<li>
<p>struct sk_buff 是 linux 网络系统中的核心结构体，linux 网络中的所有数据包的封装以及解封装都是在这个结构体的基础上进行。</p>
</li>
<li>
<p><strong>内核中 sk_buff 结构体在各层协议之间传输不是用拷贝 sk_buff 结构体，而是通过增加协议头和移动指针来操作的。</strong>如果是从 L4 传输到 L2，则是通过往 sk_buff 结构体中增加该层协议头来操作；如果是从 L4 到 L2，则是通过移动 sk_buff 结构体中的 data 指针来实现，不会删除各层协议头。这样做是为了提高 CPU 的工作效率。</p>
</li>
</ul>
<h5 id="71">7.1 详细代码</h5>
<ul>
<li><code>include/linux/skbuff.h</code>(sk_buff 结构定义和 sk_buff 宏)</li>
</ul>
<pre><code>struct sk_buff {
    /* These two members must be first. */
    struct sk_buff      *next;
    struct sk_buff      *prev;

    struct sock     *sk;
    struct skb_timeval  tstamp;
    struct net_device   *dev;
    struct net_device   *input_dev;

    union {
        struct tcphdr   *th;
        struct udphdr   *uh;
        struct icmphdr  *icmph;
        struct igmphdr  *igmph;
        struct iphdr    *ipiph;
        struct ipv6hdr  *ipv6h;
        unsigned char   *raw;
    } h;

    union {
        struct iphdr    *iph;
        struct ipv6hdr  *ipv6h;
        struct arphdr   *arph;
        unsigned char   *raw;
    } nh;

    union {
        unsigned char   *raw;
    } mac;

    struct  dst_entry   *dst;
    struct  sec_path    *sp;

    ....// 更多成员
}
</code></pre>
<ul>
<li>
<p><a href="https://blog.csdn.net/sunlei0625/article/details/61666380">struct sk_buff 成员含义</a></p>
</li>
<li>
<p>内核显然需要一个数据结构来表示报文，这个结构就是 sk_buff ( socket buffer 的简称)，它等同于在<code>&lt;TCP/IP详解 卷2&gt;</code>中描述的 BSD 内核中的 mbuf。</p>
</li>
<li>
<p>sk_buff 结构自身并不存储报文内容，它通过多个指针指向真正的报文内存空间:</p>
</li>
</ul>
<p><img alt="png" src="../../img/WechatIMG534.jpg" /></p>
<ul>
<li>sk_buff 是一个贯穿整个协议栈层次的结构，在各层间传递时，内核只需要调整 sk_buff 中的指针位置就行。</li>
</ul>
<h5 id="72-sk_buff_head">7.2 sk_buff_head</h5>
<ul>
<li>其实就是一个双向链表</li>
</ul>
<pre><code>
struct sk_buff_head
{
    struct sk_buff *next;
    struct sk_buff *prev;

    __u32 qlen;
    spinlock_t lock;
}


</code></pre>
<ul>
<li>
<ul>
<li><code>struct sock</code>中的<code>sk_write_queue</code>成员, 代表<strong>发送队列</strong>, 就是一个<code>struct sk_buff_head</code>.</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li><code>@sk_write_queue: Packet sending queue</code></li>
</ul>
</li>
</ul>
</li>
<li>
<ul>
<li><code>struct sock</code>中的<code>sk_receive_queue</code>成员, 代表<strong>接收队列</strong>, 也是一个<code>struct sk_buff_head</code>.</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li><code>@sk_receive_queue: incoming packets</code></li>
</ul>
</li>
</ul>
</li>
<li>
<ul>
<li><code>include/net/sock.h</code></li>
</ul>
</li>
</ul>
<pre><code>struct sock {
    ...
    struct sk_buff_head sk_write_queue;
    struct sk_buff_head sk_receive_queue;
    ...
}
</code></pre>
<h4 id="8-struct-proto-tcp_prot">8 struct proto tcp_prot</h4>
<p>在发送数据之前，我们往往还需要一个已经建立好连接的 socket。</p>
<p>我们就以开篇服务器缩微源代码中提到的 accept 为例，当 accept 之后，进程会创建一个新的 socket 出来，然后把它放到当前进程的打开文件列表中，专门用于和对应的客户端通信。</p>
<p>假设服务器进程通过 accept 和客户端建立了两条连接，我们来简单看一下这两条连接和进程的关联关系。</p>
<p><img alt="png" src="../../img/2F8E69F6-03A6-4704-A9F4-7692DE382258.png" /></p>
<p>其中代表一条连接的 socket 内核对象更为具体一点的结构图如下。</p>
<p><img alt="png" src="../../img/661EF808-6EFF-4483-85D9-5C65C76B33AF.png" /></p>
<p>Linux 内核网络层实现时, 总是分为三层:</p>
<ul>
<li>
<ul>
<li>
<ul>
<li><strong>BSD socket 对象</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li><strong>inet 协议族</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li><strong>具体协议,比如 tcp</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<ul>
<li>按照经典的<strong>描述符</strong>+<strong>ops</strong>思想, 对应<strong>inet socket</strong>的操作能力是用结构体<code>struct proto_ops</code>表示, 比如 inet 层一些操作是用<code>struct proto_ops inet_stream_ops</code>结构体表示, 具体<strong>协议层 sock</strong>的操作能力是用结构体<code>struct proto</code>表示, 比如 tcp 层一些操作是用<code>struct proto tcp_prot</code>结构体表示.</li>
</ul>
</li>
<li>
<ul>
<li><code>struct proto udp_prot = {</code> 定义在 <code>net/ipv4/udp.c:2650</code></li>
</ul>
</li>
<li>
<ul>
<li><code>struct proto tcp_prot = {</code> 定义在 <code>net/ipv4/tcp_ipv4.c:2447</code></li>
</ul>
</li>
</ul>
<pre><code>net/ipv4/tcp_ipv4.c


struct proto tcp_prot = {
    .name           = &quot;TCP&quot;,
    .owner          = THIS_MODULE,
    .close          = tcp_close,
    .connect        = tcp_v4_connect,
    .disconnect     = tcp_disconnect,
    .accept         = inet_csk_accept,
    .ioctl          = tcp_ioctl,
    .init           = tcp_v4_init_sock,
    .destroy        = tcp_v4_destroy_sock,
    .shutdown       = tcp_shutdown,
    .setsockopt     = tcp_setsockopt,
    .getsockopt     = tcp_getsockopt,
    .recvmsg        = tcp_recvmsg,
    .sendmsg        = tcp_sendmsg,
    .sendpage       = tcp_sendpage,
    .backlog_rcv        = tcp_v4_do_rcv,
    .release_cb     = tcp_release_cb,
    .hash           = inet_hash,
    .unhash         = inet_unhash,
    .get_port       = inet_csk_get_port,
    .enter_memory_pressure  = tcp_enter_memory_pressure,
    .stream_memory_free = tcp_stream_memory_free,
    .sockets_allocated  = &amp;tcp_sockets_allocated,
    .orphan_count       = &amp;tcp_orphan_count,
    .memory_allocated   = &amp;tcp_memory_allocated,
    .memory_pressure    = &amp;tcp_memory_pressure,
    .sysctl_mem     = sysctl_tcp_mem,
    .sysctl_wmem        = sysctl_tcp_wmem,
    .sysctl_rmem        = sysctl_tcp_rmem,
    .max_header     = MAX_TCP_HEADER,
    .obj_size       = sizeof(struct tcp_sock),
    .slab_flags     = SLAB_DESTROY_BY_RCU,
    .twsk_prot      = &amp;tcp_timewait_sock_ops,
    .rsk_prot       = &amp;tcp_request_sock_ops,
    .h.hashinfo     = &amp;tcp_hashinfo,
    .no_autobind        = true,
#ifdef CONFIG_COMPAT
    .compat_setsockopt  = compat_tcp_setsockopt,
    .compat_getsockopt  = compat_tcp_getsockopt,
#endif
#ifdef CONFIG_MEMCG_KMEM
    .init_cgroup        = tcp_init_cgroup,
    .destroy_cgroup     = tcp_destroy_cgroup,
    .proto_cgroup       = tcp_proto_cgroup,
#endif
};


</code></pre>
<ul>
<li>
<ul>
<li><code>struct proto {</code> 定义在 <code>include/net/sock.h:1048</code></li>
</ul>
</li>
</ul>
<pre><code>net/ipv4/af_inet.c

const struct proto_ops inet_stream_ops = {
    .family        = PF_INET,
    .owner         = THIS_MODULE,
    .release       = inet_release,
    .bind          = inet_bind,
    .connect       = inet_stream_connect,
    .socketpair    = sock_no_socketpair,
    .accept        = inet_accept,
    .getname       = inet_getname,
    .poll          = tcp_poll,
    .ioctl         = inet_ioctl,
    .listen        = inet_listen,
    .shutdown      = inet_shutdown,
    .setsockopt    = sock_common_setsockopt,
    .getsockopt    = sock_common_getsockopt,
    .sendmsg       = inet_sendmsg,
    .recvmsg       = inet_recvmsg,
    .mmap          = sock_no_mmap,
    .sendpage      = inet_sendpage,
    .splice_read       = tcp_splice_read,
#ifdef CONFIG_COMPAT
    .compat_setsockopt = compat_sock_common_setsockopt,
    .compat_getsockopt = compat_sock_common_getsockopt,
    .compat_ioctl      = inet_compat_ioctl,
#endif
};

</code></pre>
<h4 id="81-socket-inet-proto">8.1 关于 socket 和 inet 和 proto 的说明</h4>
<p>I think the question is the most headache problem when a newbie try to create a new protocol</p>
<ul>
<li>
<p><strong>Explain</strong>: Both structures have member elements with similar names although they represent different functions</p>
</li>
<li>
<ul>
<li>struct proto_ops: used for communication between socket layer and transport layer</li>
</ul>
</li>
<li>
<ul>
<li>struct proto: used for communicate with system calls</li>
</ul>
</li>
<li>
<p><strong>Example</strong>: when you call a system call in userspace, ex connect(), prot_ops_connect() will be call first.</p>
</li>
</ul>
<p>In fucntion <code>prot_ops_connect()</code>, we need to call <code>sk-&gt;sk_prot-&gt;connect()</code>
And <code>sk-&gt;sk_prot-&gt;connect()</code> will call <code>proto_connect()</code> automatically</p>
<p>也就是说其实分两层, struct proto_ops 用于 socket 层和传输层交互. proto 用于方便实现系统调用.</p>
<ul>
<li>
<p><img alt="png" src="../../img/47719973-7495-4457-A422-865A94BF7AB0.png" /></p>
</li>
<li>
<ul>
<li>另外, Linux 内核网络层实现时, 总是分为三层:</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li><strong>BSD socket 对象</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li><strong>inet 协议族</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<ul>
<li>
<ul>
<li><strong>具体协议,比如 tcp</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p>参考<a href="https://stackoverflow.com/questions/25656846/understand-struct-proto-and-struct-proto-ops-in-the-kernel">understand 'struct proto' and 'struct proto_ops' in the kernel</a></p>
</li>
</ul>
<h4 id="9-tcp-sock">9  tcp sock 对象的发送队列是一个链表</h4>
<p>skb 是 struct sk_buff 对象的简称，用户的发送队列就是该对象组成的一个链表。</p>
<p><img alt="png" src="../../img/9EA77EDB-4091-41DC-8A8A-902CA205D6C2.png" /></p>
<ul>
<li>另外针对服务端、针对其监听的某个tcp端口，有全连接和半连接队列的概念.</li>
<li>针对具体某个tcp sock对象, 有sk_write_queue和sk_receive_queue概念.</li>
</ul>
<h4 id="10-struct-sock">10 struct sock 讲解</h4>
<ul>
<li>struct socket 是在虚拟文件系统上被创建出来的，可以把它看成一个文件。</li>
<li>struct sock 是网络层对于 socket 的表示，结构体比较庞大.</li>
</ul>
<h5 id="101-socket-sock">10.1 socket 和 sock 是同一事物的两个侧面，<a href="https://abcdxyzk.github.io/blog/2015/06/12/kernel-net-sock-socket/">为什么不把两个数据结构合并成一个呢?</a></h5>
<ul>
<li>这是因为 socket 是 inode 结构中的一部分，即把 inode 结 构内部的一个 union 用作 socket 结构。由于插口操作的特殊性，这个数据结构中需要有大量的结构成分，如果把这些成分全部放到 socket 结构中，则 inode 结构中的这个 union 就会变得很大，从而 inode 结构也会变得很大，而对于其他文件系统这个 union 是不需要这么大的， 所以会造成巨大浪费，系统中使用 inode 结构的数量要远远超过使用 socket 的数量，故解决的办法就是把插口分成两部分，把与文件系 统关系密切的放在 socket 结构中，把与通信关系密切的放在另一个单独结构 sock 中；</li>
</ul>
<pre><code>struct inode {
    .....................
    union {
        struct ext2_inode_info ext2_i;
        struct ext3_inode_info ext3_i;
        struct socket socket_i;
        .....................
    } u;
};

</code></pre>
<pre><code>struct socket
{
    socket_state state;      // 该state用来表明该socket的当前状态
    typedef enum {
        SS_FREE = 0,         /* not allocated */
        SS_UNCONNECTED,      /* unconnected to any socket */
        SS_CONNECTING,       /* in process of connecting */
        SS_CONNECTED,        /* connected to socket */
        SS_DISCONNECTING     /* in process of disconnecting */
    } socket_state;
    unsigned long flags;     //该成员可能的值如下，该标志用来设置socket是否正在忙碌
    #define SOCK_ASYNC_NOSPACE 0
    #define SOCK_ASYNC_WAITDATA 1
    #define SOCK_NOSPACE 2
    struct proto_ops *ops;   //依据协议邦定到该socket上的特定的协议族的操作函数指针，例如IPv4 TCP就是inet_stream_ops
    struct inode *inode;     //表明该socket所属的inode
    struct fasync_struct *fasync_list; //异步唤醒队列
    struct file *file;       //file回指指针
    struct sock *sk;         //sock指针
    wait_queue_head_t wait;  //sock的等待队列，在TCP需要等待时就sleep在这个队列上
    short type;              //表示该socket在特定协议族下的类型例如SOCK_STREAM,
    unsigned char passcred;  //在TCP分析中无须考虑
};




/**
  * struct sock - network layer representation of sockets
  * @__sk_common: shared layout with inet_timewait_sock
  * @sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
  * @sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
  * @sk_lock:   synchronizer
  * @sk_rcvbuf: size of receive buffer in bytes
  * @sk_wq: sock wait queue and async head
  * @sk_rx_dst: receive input route used by early demux
  * @sk_dst_cache: destination cache
  * @sk_dst_lock: destination cache lock
  * @sk_policy: flow policy
  * @sk_receive_queue: incoming packets
  * @sk_wmem_alloc: transmit queue bytes committed
  * @sk_write_queue: Packet sending queue
  * @sk_async_wait_queue: DMA copied packets
  * @sk_omem_alloc: &quot;o&quot; is &quot;option&quot; or &quot;other&quot;
  * @sk_wmem_queued: persistent queue size
  * @sk_forward_alloc: space allocated forward
  * @sk_napi_id: id of the last napi context to receive data for sk
  * @sk_ll_usec: usecs to busypoll when there is no data
  * @sk_allocation: allocation mode
  * @sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
  * @sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
  * @sk_sndbuf: size of send buffer in bytes
  * @sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
  *        %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
  * @sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
  * @sk_no_check_rx: allow zero checksum in RX packets
  * @sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
  * @sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
  * @sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
  * @sk_gso_max_size: Maximum GSO segment size to build
  * @sk_gso_max_segs: Maximum number of GSO segments
  * @sk_lingertime: %SO_LINGER l_linger setting
  * @sk_backlog: always used with the per-socket spinlock held
  * @sk_callback_lock: used with the callbacks in the end of this struct
  * @sk_error_queue: rarely used
  * @sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt,
  *           IPV6_ADDRFORM for instance)
  * @sk_err: last error
  * @sk_err_soft: errors that don't cause failure but are the cause of a
  *           persistent failure not just 'timed out'
  * @sk_drops: raw/udp drops counter
  * @sk_ack_backlog: current listen backlog
  * @sk_max_ack_backlog: listen backlog set in listen()
  * @sk_priority: %SO_PRIORITY setting
  * @sk_cgrp_prioidx: socket group's priority map index
  * @sk_type: socket type (%SOCK_STREAM, etc)
  * @sk_protocol: which protocol this socket belongs in this network family
  * @sk_peer_pid: &amp;struct pid for this socket's peer
  * @sk_peer_cred: %SO_PEERCRED setting
  * @sk_rcvlowat: %SO_RCVLOWAT setting
  * @sk_rcvtimeo: %SO_RCVTIMEO setting
  * @sk_sndtimeo: %SO_SNDTIMEO setting
  * @sk_rxhash: flow hash received from netif layer
  * @sk_filter: socket filtering instructions
  * @sk_protinfo: private area, net family specific, when not using slab
  * @sk_timer: sock cleanup timer
  * @sk_stamp: time stamp of last packet received
  * @sk_socket: Identd and reporting IO signals
  * @sk_user_data: RPC layer private data
  * @sk_frag: cached page frag
  * @sk_peek_off: current peek_offset value
  * @sk_send_head: front of stuff to transmit
  * @sk_security: used by security modules
  * @sk_mark: generic packet mark
  * @sk_classid: this socket's cgroup classid
  * @sk_cgrp: this socket's cgroup-specific proto data
  * @sk_write_pending: a write to stream socket waits to start
  * @sk_state_change: callback to indicate change in the state of the sock
  * @sk_data_ready: callback to indicate there is data to be processed
  * @sk_write_space: callback to indicate there is bf sending space available
  * @sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
  * @sk_backlog_rcv: callback to process the backlog
  * @sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
 */
struct sock {
    /*
     * Now struct inet_timewait_sock also uses sock_common, so please just
     * don't add nothing before this first member (__sk_common) --acme
     */
    struct sock_common  __sk_common;
#define sk_node         __sk_common.skc_node
#define sk_nulls_node       __sk_common.skc_nulls_node
#define sk_refcnt       __sk_common.skc_refcnt
#define sk_tx_queue_mapping __sk_common.skc_tx_queue_mapping

#define sk_dontcopy_begin   __sk_common.skc_dontcopy_begin
#define sk_dontcopy_end     __sk_common.skc_dontcopy_end
#define sk_hash         __sk_common.skc_hash
#define sk_portpair     __sk_common.skc_portpair
#define sk_num          __sk_common.skc_num
#define sk_dport        __sk_common.skc_dport
#define sk_addrpair     __sk_common.skc_addrpair
#define sk_daddr        __sk_common.skc_daddr
#define sk_rcv_saddr        __sk_common.skc_rcv_saddr
#define sk_family       __sk_common.skc_family
#define sk_state        __sk_common.skc_state
#define sk_reuse        __sk_common.skc_reuse
#define sk_reuseport        __sk_common.skc_reuseport
#define sk_bound_dev_if     __sk_common.skc_bound_dev_if
#define sk_bind_node        __sk_common.skc_bind_node
#define sk_prot         __sk_common.skc_prot
#define sk_net          __sk_common.skc_net
#define sk_v6_daddr     __sk_common.skc_v6_daddr
#define sk_v6_rcv_saddr __sk_common.skc_v6_rcv_saddr

    socket_lock_t       sk_lock;
    struct sk_buff_head sk_receive_queue;
    /*
     * The backlog queue is special, it is always used with
     * the per-socket spinlock held and requires low latency
     * access. Therefore we special case it's implementation.
     * Note : rmem_alloc is in this structure to fill a hole
     * on 64bit arches, not because its logically part of
     * backlog.
     */
    struct {
        atomic_t    rmem_alloc;
        int     len;
        struct sk_buff  *head;
        struct sk_buff  *tail;
    } sk_backlog;
#define sk_rmem_alloc sk_backlog.rmem_alloc
    int         sk_forward_alloc;
#ifdef CONFIG_RPS
    __u32           sk_rxhash;
#endif
#ifdef CONFIG_NET_RX_BUSY_POLL
    unsigned int        sk_napi_id;
    unsigned int        sk_ll_usec;
#endif
    atomic_t        sk_drops;
    int         sk_rcvbuf;

    struct sk_filter __rcu  *sk_filter;
    struct socket_wq __rcu  *sk_wq;

#ifdef CONFIG_NET_DMA
    struct sk_buff_head sk_async_wait_queue;
#endif

#ifdef CONFIG_XFRM
    struct xfrm_policy  *sk_policy[2];
#endif
    unsigned long       sk_flags;
    struct dst_entry    *sk_rx_dst;
    struct dst_entry __rcu  *sk_dst_cache;
    spinlock_t      sk_dst_lock;
    atomic_t        sk_wmem_alloc;
    atomic_t        sk_omem_alloc;
    int         sk_sndbuf;
    struct sk_buff_head sk_write_queue;
    kmemcheck_bitfield_begin(flags);
    unsigned int        sk_shutdown  : 2,
                sk_no_check_tx : 1,
                sk_no_check_rx : 1,
                sk_userlocks : 4,
                sk_protocol  : 8,
                sk_type      : 16;
#define SK_PROTOCOL_MAX U8_MAX
    kmemcheck_bitfield_end(flags);
    int         sk_wmem_queued;
    gfp_t           sk_allocation;
    u32         sk_pacing_rate; /* bytes per second */
    u32         sk_max_pacing_rate;
    netdev_features_t   sk_route_caps;
    netdev_features_t   sk_route_nocaps;
    int         sk_gso_type;
    unsigned int        sk_gso_max_size;
    u16         sk_gso_max_segs;
    int         sk_rcvlowat;
    unsigned long           sk_lingertime;
    struct sk_buff_head sk_error_queue;
    struct proto        *sk_prot_creator;
    rwlock_t        sk_callback_lock;
    int         sk_err,
                sk_err_soft;
    unsigned short      sk_ack_backlog;
    unsigned short      sk_max_ack_backlog;
    __u32           sk_priority;
#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
    __u32           sk_cgrp_prioidx;
#endif
    struct pid      *sk_peer_pid;
    const struct cred   *sk_peer_cred;
    long            sk_rcvtimeo;
    long            sk_sndtimeo;
    void            *sk_protinfo;
    struct timer_list   sk_timer;
    ktime_t         sk_stamp;
    struct socket       *sk_socket;
    void            *sk_user_data;
    struct page_frag    sk_frag;
    struct sk_buff      *sk_send_head;
    __s32           sk_peek_off;
    int         sk_write_pending;
#ifdef CONFIG_SECURITY
    void            *sk_security;
#endif
    __u32           sk_mark;
    u32         sk_classid;
    struct cg_proto     *sk_cgrp;
    void            (*sk_state_change)(struct sock *sk);
    void            (*sk_data_ready)(struct sock *sk);
    void            (*sk_write_space)(struct sock *sk);
    void            (*sk_error_report)(struct sock *sk);
    int         (*sk_backlog_rcv)(struct sock *sk,
                          struct sk_buff *skb);
    void                    (*sk_destruct)(struct sock *sk);
};


</code></pre>
<h4 id="socket-sock">socket &amp; sock</h4>
<ul>
<li>
<p>用户空间通过 socket()、bind()、listen()、accept() 等库函数进行网络编程。而这里提到的 socket 和 sock 是内核中的两个数据结构，其中 socket 向上面向用户，而 sock 向下面向协议栈。</p>
</li>
<li>
<p>如下图所示，这两个结构实际上是一一对应的。</p>
</li>
</ul>
<p><img alt="png" src="../../img/WechatIMG536.jpg" /></p>
<ul>
<li>
<p>注意到，这两个结构上都有一个叫 ops 的指针, 但它们的类型不同。socket 的 ops 是一个指向 struct proto_ops 的指针，sock 的 ops 是一个指向 struct proto 的指针, 它们在结构被创建时确定。</p>
</li>
<li>
<p>回忆网络编程中 socket() 函数的原型：</p>
</li>
</ul>
<pre><code>#include &lt;sys/socket.h&gt;

sockfd = socket(int socket_family, int socket_type, int protocol);
</code></pre>
<ul>
<li>
<p>实际上, socket-&gt;ops 和 sock-&gt;ops 由前两个参数 socket_family 和 socket_type 共同确定。</p>
</li>
<li>
<p>如果 socket_family 是最常用的 PF_INET 协议簇, 则 socket-&gt;ops 和 sock-&gt;ops 的取值就记录在 INET 协议开关表中</p>
</li>
</ul>
<pre><code>

static struct inet_protosw inetsw_array[] =
{
    {
        .type =     SOCK_STREAM,
        .protocol = IPPROTO_TCP,
        .prot =     &amp;tcp_prot,                 // 对应 sock-&gt;ops
        .ops =      &amp;inet_stream_ops,          // 对应 socket-&gt;ops
        .flags =    INET_PROTOSW_PERMANENT | INET_PROTOSW_ICSK,
    },

    {
        .type =     SOCK_DGRAM,
        .protocol = IPPROTO_UDP,
        .prot =     &amp;udp_prot,                 // 对应 sock-&gt;ops
        .ops =      &amp;inet_dgram_ops,           // 对应 socket-&gt;ops
        .flags =    INET_PROTOSW_PERMANENT,
    },
}
.......


</code></pre>
<h4 id="102-struct-inet_connection_sock">10.2 struct inet_connection_sock</h4>
<p><img alt="png" src="../../img/WechatIMG424.jpg" /></p>
<p><img alt="png" src="../../img/928E82B6-9FCF-4C97-A9AF-F7218C5933CF.jpg" /></p>
<pre><code>include/net/inet_connection_sock.h

/** inet_connection_sock - INET connection oriented sock
 *
 * @icsk_accept_queue:     FIFO of established children
 * @icsk_bind_hash:    Bind node
 * @icsk_timeout:      Timeout
 * @icsk_retransmit_timer: Resend (no ack)
 * @icsk_rto:          Retransmit timeout
 * @icsk_pmtu_cookie       Last pmtu seen by socket
 * @icsk_ca_ops        Pluggable congestion control hook
 * @icsk_af_ops        Operations which are AF_INET{4,6} specific
 * @icsk_ca_state:     Congestion control state
 * @icsk_retransmits:      Number of unrecovered [RTO] timeouts
 * @icsk_pending:      Scheduled timer event
 * @icsk_backoff:      Backoff
 * @icsk_syn_retries:      Number of allowed SYN (or equivalent) retries
 * @icsk_probes_out:       unanswered 0 window probes
 * @icsk_ext_hdr_len:      Network protocol overhead (IP/IPv6 options)
 * @icsk_ack:          Delayed ACK control data
 * @icsk_mtup;         MTU probing control data
 */
struct inet_connection_sock {
    /* inet_sock has to be the first member! */
    struct inet_sock      icsk_inet;
    struct request_sock_queue icsk_accept_queue;
    struct inet_bind_bucket   *icsk_bind_hash;
    unsigned long         icsk_timeout;
    struct timer_list     icsk_retransmit_timer;
    struct timer_list     icsk_delack_timer;
    __u32             icsk_rto;
    __u32             icsk_pmtu_cookie;
    const struct tcp_congestion_ops *icsk_ca_ops;
    const struct inet_connection_sock_af_ops *icsk_af_ops;
    unsigned int          (*icsk_sync_mss)(struct sock *sk, u32 pmtu);
    __u8              icsk_ca_state;
    __u8              icsk_retransmits;
    __u8              icsk_pending;
    __u8              icsk_backoff;
    __u8              icsk_syn_retries;
    __u8              icsk_probes_out;
    __u16             icsk_ext_hdr_len;
    struct {
        __u8          pending;   /* ACK is pending             */
        __u8          quick;     /* Scheduled number of quick acks     */
        __u8          pingpong;  /* The session is interactive         */
        __u8          blocked;   /* Delayed ACK was blocked by socket lock */
        __u32         ato;       /* Predicted tick of soft clock       */
        unsigned long     timeout;   /* Currently scheduled timeout        */
        __u32         lrcvtime;  /* timestamp of last received data packet */
        __u16         last_seg_size; /* Size of last incoming segment      */
        __u16         rcv_mss;   /* MSS used for delayed ACK decisions     */
    } icsk_ack;
    struct {
        int       enabled;

        /* Range of MTUs to search */
        int       search_high;
        int       search_low;

        /* Information on the current probe. */
        int       probe_size;
    } icsk_mtup;
    u32           icsk_ca_priv[16];
    u32           icsk_user_timeout;
#define ICSK_CA_PRIV_SIZE   (16 * sizeof(u32))
};

</code></pre>
<h4 id="103-struct-inet_sock">10.3 struct inet_sock</h4>
<pre><code>
/** struct inet_sock - representation of INET sockets
 *
 * @sk - ancestor class
 * @pinet6 - pointer to IPv6 control block
 * @inet_daddr - Foreign IPv4 addr
 * @inet_rcv_saddr - Bound local IPv4 addr
 * @inet_dport - Destination port
 * @inet_num - Local port
 * @inet_saddr - Sending source
 * @uc_ttl - Unicast TTL
 * @inet_sport - Source port
 * @inet_id - ID counter for DF pkts
 * @tos - TOS
 * @mc_ttl - Multicasting TTL
 * @is_icsk - is this an inet_connection_sock?
 * @uc_index - Unicast outgoing device index
 * @mc_index - Multicast device index
 * @mc_list - Group array
 * @cork - info to build ip hdr on each ip frag while socket is corked
 */
struct inet_sock {
    /* sk and pinet6 has to be the first two members of inet_sock */
    struct sock     sk;
#if IS_ENABLED(CONFIG_IPV6)
    struct ipv6_pinfo   *pinet6;
#endif
    /* Socket demultiplex comparisons on incoming packets. */
#define inet_daddr      sk.__sk_common.skc_daddr
#define inet_rcv_saddr      sk.__sk_common.skc_rcv_saddr
#define inet_dport      sk.__sk_common.skc_dport
#define inet_num        sk.__sk_common.skc_num

    __be32          inet_saddr;
    __s16           uc_ttl;
    __u16           cmsg_flags;
    __be16          inet_sport;
    __u16           inet_id;

    struct ip_options_rcu __rcu *inet_opt;
    int         rx_dst_ifindex;
    __u8            tos;
    __u8            min_ttl;
    __u8            mc_ttl;
    __u8            pmtudisc;
    __u8            recverr:1,
                is_icsk:1,
                freebind:1,
                hdrincl:1,
                mc_loop:1,
                transparent:1,
                mc_all:1,
                nodefrag:1;
    __u8            rcv_tos;
    int         uc_index;
    int         mc_index;
    __be32          mc_addr;
    struct ip_mc_socklist __rcu *mc_list;
    struct inet_cork_full   cork;
};


</code></pre>
<h4 id="104">10.4 半连接哈希表和全连接队列(半连接队列和全连接队列)</h4>
<p><img alt="png" src="../../img/WechatIMG425.jpg" /></p>
<ul>
<li><strong>半连接哈希表</strong>是采用<code>struct lisen_sock</code>结构体表示</li>
<li><strong>全连接队列</strong>采用 rskq_accept_head 和 rskq_accept_tail 指针分别指向头部和尾部</li>
<li>其底层元素都是<code>struct request_sock</code>, 全连接队列就是一个链表</li>
<li><strong>半连接哈希表</strong>和<strong>全链接队列</strong>被<strong>一起</strong>封装在<strong>struct requst_sock_queue</strong>当中</li>
<li>内核世界中，哈希表就是数组, 因为数组有下标，无论多大都可以快速找到某个指定元素. </li>
<li>半连接队列就是一个<code>struct request_sock</code>组成的数组，全连接队列就是一个<code>struct request_sock</code>组成的链表</li>
</ul>
<h4 id="105-struct-request_sock-mini-sock-to-represent-a-connection-request">10.5 struct request_sock - mini sock to represent a connection request</h4>
<pre><code>
struct tcp_request_sock {
    struct inet_request_sock req;

#ifdef CONFIG_TCP_MD5SIG
    /* Only used by TCP MD5 Signature so far. */
    const struct tcp_request_sock_ops *af_specific;
#endif
    //客户端SYN段中携带的seq，即客户端的初始序列号 */
    u32 rcv_isn;
    //SYN+ACK段携带的seq，即服务器端的初始序列号
    u32 snt_isn;
    //SYN+ACK段发送的时间戳，基于jiffies
    u32 snt_synack;
};

struct inet_request_sock {
    struct request_sock req;

#if IS_ENABLED(CONFIG_IPV6)
    u16 inet6_rsk_offset;
#endif

    __be16 loc_port; /* 服务器端端口号 */
    __be32 loc_addr; /* 服务器端IP地址 */
    __be32 rmt_addr; /* 客户端IP地址 */
    __be16 rmt_port; /* 客户端端口号 */

    kmemcheck_bitfield_begin(flags);
    u16 snd_wscale : 4, /* 客户端的窗口扩大因子 */
        rcv_wscale : 4, /* 服务器端的窗口扩大因子 */
        tstamp_ok : 1, /* 标识本连接是否支持TIMESTAMP选项 */
        sack_ok : 1, /* 标识本连接是否支持SACK选项 */
        wscale_ok : 1, /* 标识本连接是否支持Window Scale选项 */
        ecn_ok : 1, /* 标识本连接是否支持ECN选项 */
        acked : 1,
        no_srccheck : 1;
    kmemcheck_bitfield_end(flags);

    struct ip_options_rcu *opt; /* IP选项 */
};

/* struct request_sock - mini sock to represent a connection request
 */
struct request_sock {
    //和其它struct request_sock对象形成链表
    struct request_sock     *dl_next; /* Must be first member! */
    //SYN段中客户端通告的MSS
    u16             mss;
    //SYN+ACK段已经重传的次数，初始化为0
    u8              retrans;
    u8              __pad;
    u32             window_clamp; /* window clamp at creation time */
    u32             rcv_wnd;      /* rcv_wnd offered first time */
    u32             ts_recent;
    //SYN+ACK段的超时时间
    unsigned long           expires;
    //指向tcp_request_sock_ops,该函数集用于处理第三次握手的
    //ACK段以及后续accept过程中struct tcp_sock对象的创建
    const struct request_sock_ops   *rsk_ops;
    //连接建立前无效，建立后指向创建的tcp_sock结构
    struct sock         *sk;
    u32             secid;
    u32             peer_secid;
};


</code></pre>
<ul>
<li><a href="https://blog.csdn.net/wangquan1992/article/details/108885613">linux 内核协议栈 TCP 之连接请求队列</a></li>
</ul>
<h4 id="11-linux-softnet_data">11 linux 实现软中断的重要数据结构 softnet_data</h4>
<ul>
<li>是用于实现软中断的重要数据结构,softnet_data 是每个 CPU 都拥有的一个数据结构.</li>
</ul>
<blockquote>
<p>以下是基于 2.6.32 的源码</p>
</blockquote>
<pre><code>struct softnet_data

{

    struct Qdisc        *output_queue;  //发送帧队列

    struct sk_buff_head input_pkt_queue;  //接收帧队列（入口队列）

    struct list_head    poll_list; //这是一个双向链表

    struct sk_buff      *completion_queue;



    struct napi_struct  backlog;

};

</code></pre>
<p><a href="https://developer.aliyun.com/article/11018">说明</a>：</p>
<ol>
<li>
<p>可以看到发送帧队列并不是 skb 的链表，而是 Qdisc 的链表，这是因为发送一般需要 Qos 流控，所以发送帧会存入相应 dev 关联的 Qdisc 中（Qdisc 中有 skb 的队列），详见“后面链路层数据包发送”分析。</p>
</li>
<li>
<p>poll_list 是一个双向链表，每一个节点是一个 napi_struct 结构，而 napi_struct 又是 net_device 的成员，所以这个链表也可以理解为一个 net_device 链表，这些 net_device 都带有输入帧等着被处理。This is a bidirectional list of devices with input frames waiting to be processed. More details can be found in the section "Processing the NET_RX_SOFTIRQ: net_rx_action" in Chapter 10.</p>
</li>
<li>
<p>input_pkt_queue 是设备驱动将数据从物理介质接收后封装成 skb 后存放的缓存队列，所有<strong>非 NAPI 设备</strong>共有这一个输入缓存队列，而 NAPI 设备有自己的私有队列用于存放输入包。 This queue, initialized in <code>net_dev_init</code>, is where incoming frames are stored before being processed by the driver. It is used by non-NAPI drivers; those that have been upgraded to NAPI use their own private queues.</p>
</li>
<li>
<p><a href="http://www.embeddedlinux.org.cn/linux_net/0596002556/understandlni-CHP-9-SECT-4.html">softnet_data Structure</a></p>
</li>
</ol>
<blockquote>
<p>以下基于 3.16.51 源代码</p>
</blockquote>
<pre><code>
/*
 * Incoming packets are placed on per-cpu queues
 */
struct softnet_data {
    struct Qdisc        *output_queue;
    struct Qdisc        **output_queue_tailp;
    struct list_head    poll_list; //这里每一个元素是一个struct napi_struct, 而每个napi_struct其实是struct igb_q_vector子元素.
    struct sk_buff      *completion_queue;
    struct sk_buff_head process_queue;

    /* stats */
    unsigned int        processed;
    unsigned int        time_squeeze;
    unsigned int        cpu_collision;
    unsigned int        received_rps;

#ifdef CONFIG_RPS
    struct softnet_data *rps_ipi_list;

    /* Elements below can be accessed between CPUs for RPS */
    struct call_single_data csd ____cacheline_aligned_in_smp;
    struct softnet_data *rps_ipi_next;
    unsigned int        cpu;
    unsigned int        input_queue_head;
    unsigned int        input_queue_tail;
#endif
    unsigned int        dropped;
    struct sk_buff_head input_pkt_queue;
    struct napi_struct  backlog;

#ifdef CONFIG_NET_FLOW_LIMIT
    struct sd_flow_limit __rcu *flow_limit;
#endif
};


</code></pre>
<h5 id="_4">一些重要成员</h5>
<ul>
<li>缓冲队列(发送、接收)</li>
</ul>
<h4 id="12-struct-msghdr">12 struct msghdr</h4>
<pre><code>
struct msghdr {
   void         *msg_name;       /* optional address */
   socklen_t     msg_namelen;    /* size of address */
   struct iovec *msg_iov;        /* scatter/gather array */
   size_t        msg_iovlen;     /* # elements in msg_iov */
   void         *msg_control;    /* ancillary data, see below */
   size_t        msg_controllen; /* ancillary data buffer len */
   int           msg_flags;      /* flags on received message */
};

</code></pre>
<ul>
<li>The <strong>msghdr</strong> structure is used to minimize the number of directly supplied parameters to the <code>recvmsg()</code> and <code>sendmsg()</code> functions</li>
<li>
<p>主要用于向一个 socket 发送消息，或从一个 socket 中接收消息。</p>
</li>
<li>
<ul>
<li>msghdr 结构一般会用于如下两个函数中：</li>
</ul>
</li>
</ul>
<pre><code>
#include &lt;sys/types.h&gt;
#include &lt;sys/socket.h&gt;

ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);
ssize_t recvmsg(int sockfd, struct msghdr *msg, int flags);

</code></pre>
<h4 id="13-struct-iovec">13 struct iovec</h4>
<pre><code>
struct iovec
{
    void __user *iov_base;  /* BSD uses caddr_t (1003.1g requires void *) */
    __kernel_size_t iov_len; /* Must be size_t (1003.1g) */
};

</code></pre>
<ul>
<li>看着就是用于存储用户空间的内存地址和长度. 方便后续把数据从用户空间 COPY 到内核空间</li>
</ul>
<h4 id="14">14 等待队列相关结构体</h4>
<blockquote>
<p>本质上来说, 等待队列就是一个双向链表</p>
<p>代码中使用了两个数据结构来描述一个等待队列：wait_queue_head_t 和 wait_queue_t。</p>
<p>这两个数据结构定义在 include/linux/wait.h 头文件中。</p>
</blockquote>
<ul>
<li>其一 wait_queue_t（等待队列项）</li>
</ul>
<pre><code>
typedef struct __wait_queue wait_queue_t;

struct __wait_queue {
    unsigned int        flags;
#define WQ_FLAG_EXCLUSIVE   0x01
    void            *private;
    wait_queue_func_t   func;
    struct list_head    task_list;
};


下面说明一下各个成员的作用：

flags: 可以设置为 WQ_FLAG_EXCLUSIVE，表示等待的进程应该独占资源（解决惊群现象）。
private: 一般用于保存等待进程的进程描述符 task_struct。
func: 唤醒函数，一般设置为 default_wake_function() 函数，当然也可以设置为自定义的唤醒函数。
task_list: 用于连接其他等待资源的进程。

</code></pre>
<ul>
<li>其二 wait_queue_head_t（等待队列头）</li>
</ul>
<pre><code>
struct __wait_queue_head {
    spinlock_t      lock;
    struct list_head    task_list;
};
typedef struct __wait_queue_head wait_queue_head_t;

</code></pre>
<ul>
<li>其三 创建 wait_queue_t（创建等待队列项）</li>
</ul>
<pre><code>
#define DEFINE_WAIT_FUNC(name, function)                \
    wait_queue_t name = {                       \
        .private    = current,              \
        .func       = function,             \
        .task_list  = LIST_HEAD_INIT((name).task_list), \
    }

#define DEFINE_WAIT(name) DEFINE_WAIT_FUNC(name, autoremove_wake_function)


</code></pre>
<p><img alt="png" src="../../img/WechatIMG398.jpg" /></p>
<p><img alt="png" src="../../img/WechatIMG399.jpg" /></p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1685665">Linux 等待队列原理与实现</a></li>
<li><a href="http://velep.com/archives/815.html">linux 等待队列 wait_queue_head_t 和 wait_queue_t</a></li>
</ul>
<h4 id="15-struct-eventpoll">15 struct eventpoll</h4>
<pre><code>
/*
 * This structure is stored inside the &quot;private_data&quot; member of the file
 * structure and represents the main data structure for the eventpoll
 * interface.
 */
struct eventpoll {
    /* Protect the access to this structure */
    spinlock_t lock;

    /*
     * This mutex is used to ensure that files are not removed
     * while epoll is using them. This is held during the event
     * collection loop, the file cleanup path, the epoll file exit
     * code and the ctl operations.
     */
    struct mutex mtx;

    /* Wait queue used by sys_epoll_wait() */
    wait_queue_head_t wq;

    /* Wait queue used by file-&gt;poll() */
    wait_queue_head_t poll_wait;

    /* List of ready file descriptors */
    struct list_head rdllist;

    /* RB tree root used to store monitored fd structs */
    struct rb_root rbr;

    /*
     * This is a single linked list that chains all the &quot;struct epitem&quot; that
     * happened while transferring ready events to userspace w/out
     * holding -&gt;lock.
     */
    struct epitem *ovflist;

    /* wakeup_source used when ep_scan_ready_list is running */
    struct wakeup_source *ws;

    /* The user that created the eventpoll descriptor */
    struct user_struct *user;

    struct file *file;

    /* used to optimize loop detection check */
    int visited;
    struct list_head visited_list_link;
};


</code></pre>
<h4 id="16-struct-socket_wq">16 struct socket_wq</h4>
<pre><code>
 struct socket_wq {
       /* Note: wait MUST be first field of socket_wq */
     wait_queue_head_t   wait;
     struct fasync_struct    *fasync_list;
     struct rcu_head     rcu;
 } ____cacheline_aligned_in_smp;

</code></pre>
<h4 id="17-struct-neighbour">17 struct neighbour</h4>
<ul>
<li>Neighbors are represented by <code>struct neighbour</code> structures. The structure is complex and includes status fields, virtual functions to interface with L3 protocols, timers, and cached L2 headers.</li>
</ul>
<pre><code>
include/net/neighbour.h

struct neighbour {
    struct neighbour __rcu  *next;
    struct neigh_table  *tbl;
    struct neigh_parms  *parms;
    unsigned long       confirmed;
    unsigned long       updated;
    rwlock_t        lock;
    atomic_t        refcnt;
    struct sk_buff_head arp_queue;
    unsigned int        arp_queue_len_bytes;
    struct timer_list   timer;
    unsigned long       used;
    atomic_t        probes;
    __u8            flags;
    __u8            nud_state;
    __u8            type;
    __u8            dead;
    seqlock_t       ha_lock;
    unsigned char       ha[ALIGN(MAX_ADDR_LEN, sizeof(unsigned long))];
    struct hh_cache     hh;
    int         (*output)(struct neighbour *, struct sk_buff *);
    const struct neigh_ops  *ops;
    struct rcu_head     rcu;
    struct net_device   *dev;
    u8          primary_key[0];
};

struct neigh_ops {
    int         family;
    void            (*solicit)(struct neighbour *, struct sk_buff *);
    void            (*error_report)(struct neighbour *, struct sk_buff *);
    int         (*output)(struct neighbour *, struct sk_buff *);
    int         (*connected_output)(struct neighbour *, struct sk_buff *);
};


</code></pre>
<h4 id="18-struct-inet_hashinfo-tcp_hashinfo">18 struct inet_hashinfo tcp_hashinfo</h4>
<ul>
<li>首先，在 linux 内核的网络模块里维护着一个全局实例，用来存储所有和 tcp 相关的 socket：</li>
</ul>
<pre><code>
// net/ipv4/tcp_ipv4.c
struct inet_hashinfo tcp_hashinfo;

</code></pre>
<ul>
<li>其次，在该实例的内部，又根据 socket 类型的不同，划分成四个 hashtable：</li>
</ul>
<pre><code>
// include/net/inet_hashtables.h
struct inet_hashinfo {
        // key是由本地地址、本地端口、远程地址、远程端口组成的四元组
        // value是正在建立连接或已经建立连接的socket
        // 比如，当内核收到一个tcp消息时，它先从消息头里读出地址和端口等信息
        // 然后用该信息到ehash里获取对应的socket
        // 最后把剩余的tcp数据添加到该socket的recv buf中供用户程序读取
        struct inet_ehash_bucket        *ehash;

        // key是本地端口
        // value是使用这个端口的所有socket
        // 比如，当我们用socket监听一个端口时，该socket就在bhash里
        // 同理，由该监听端口建立的连接对应的那些socket也在这里
        // 因为它们也都是使用同样的本地端口
        struct inet_bind_hashbucket     *bhash;

        // key是本地地址和端口组成的二元组
        // value是对应的处于listen状态的socket
        struct inet_listen_hashbucket   *lhash2;

        // key是本地端口
        // value是对应的处于listen状态的socket
        struct inet_listen_hashbucket   listening_hash[INET_LHTABLE_SIZE];
};

</code></pre>
<p><img alt="png" src="../../img/5a2bf78d55e946c38a06ee6e789aa4d2.png" /></p>
<ul>
<li>如何判断一个本地端口是否被使用</li>
</ul>
<pre><code>
       // 根据端口算出hash值，然后根据这个值找到bhash中对应的slot
        head = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,
                                          hinfo-&gt;bhash_size)];

        // 遍历slot指向的链表，找到port对应的值
        inet_bind_bucket_for_each(tb, &amp;head-&gt;chain)
                if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;l3mdev == l3mdev &amp;&amp;
                    tb-&gt;port == port)
                        goto tb_found;

        // 如果没找到，说明现在还没有人使用这个端口，就新创建一个
        // 新创建的实例就会放到bhash中，表明这个端口我在使用了
        tb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep,
                                     net, head, port, l3mdev);
</code></pre>
<ul>
<li>参考<a href="https://cloud.tencent.com/developer/article/1534086">Linux 系统研究 - 操作系统是如何管理 tcp 连接的 (1)</a></li>
</ul>
<h4 id="19-dst_entry">19 dst_entry</h4>
<p>内核需要确定收到的报文是应该本地上送(local deliver)还是转发(forward),对本机发送(local out)的报文需要确定是从哪个网卡发送出去，这都是内核通过查询 <strong>fib (forward information base, 转发信息表)</strong> 确定。fib 可以理解为一个数据库，数据来源是用户配置或者内核自动生成的路由。</p>
<p>fib 查询的输入是报文 sk_buff，输出是 dst_entry. dst_entry 会被设置到 skb 上：</p>
<pre><code>static inline void skb_dst_set(struct sk_buff *skb, struct dst_entry *dst)
{
    skb-&gt;_skb_refdst = (unsigned long)dst;
}
</code></pre>
<p>而 dst_entry 中最重要的是一个 input 指针和 output 指针：</p>
<pre><code>struct dst_entry
{
    ......
    int (*input)(struct sk_buff *);
    int (*output)(struct net *net, struct sock *sk, struct sk_buff *skb);
    ......
}
</code></pre>
<p>对于需要本机上送的报文：</p>
<pre><code>rth-&gt;dst.input = ip_local_deliver;
</code></pre>
<p>对需要转发的报文：</p>
<pre><code>rth-&gt;dst.input = ip_forward;
</code></pre>
<p>对本机发送的报文：</p>
<pre><code>rth-&gt;dst.output = ip_output;
</code></pre>
<h3 id="_5">(二) 核心函数</h3>
<h4 id="l3-l4">L3-&gt;L4</h4>
<p>我们知道网络协议栈是分层的，但实际上，具体到实现，内核协议栈的分层只是逻辑上的，本质还是函数调用。发送流程(上层调用下层)通常是直接调用(因为没有不确定性，比如 TCP 知道下面一定 IP)，但接收过程不一样了，比如报文在 IP 层时，它上面可能是 TCP，也可能是 UDP，或者是 ICMP 等等，所以接收过程使用的是<strong>注册-回调</strong><a href="https://mp.weixin.qq.com/s/DD8EN_BiAAOukKIioFNm1w">机制</a>。</p>
<p>还是以 INET 协议簇为例，注册接口是：</p>
<pre><code>int inet_add_protocol(const struct net_protocol *prot, unsigned char protocol);
</code></pre>
<p>在内核网络子系统初始化时，L4 层协议(如下面的 TCP 和 UDP)会被注册：</p>
<pre><code>static struct net_protocol tcp_protocol = {
    ......
    .handler = tcp_v4_rcv,
    ......
};

</code></pre>
<pre><code>static struct net_protocol udp_protocol = {
    .....
    .handler = udp_rcv,
    .....
};
.......

</code></pre>
<p>而在 IP 层，查询过路由后，如果该报文是需要上送本机的，则会根据报文的 L4 协议，送给不同的 L4 处理：</p>
<pre><code>static int ip_local_deliver_finish(struct net *net, struct sock *sk, struct sk_buff *skb)
{
    ......
    ipprot = rcu_dereference(inet_protos[protocol]);
    ......
    ret = ipprot-&gt;handler(skb);
    ......
}
.......
</code></pre>
<h4 id="l2-l3">L2-&gt;L3</h4>
<p>L2-&gt;L3 如出一辙。只不过注册接口变成了：</p>
<pre><code>void dev_add_pack(struct packet_type *pt)
</code></pre>
<p>谁会注册呢？显然至少 IP 会：</p>
<pre><code>static struct packet_type ip_packet_type = {
    .type = cpu_to_be16(ETH_P_IP),
    .func = ip_rcv,
}
.......

</code></pre>
<p>而在报文接收过程中，设备驱动程序会将报文的 L3 类型设置到 skb-&gt;protocol，然后在内核 netif_receive_skb 收包时，会根据这个 protocol 调用不同的回调函数：</p>
<pre><code>__netif_receive_skb(struct sk_buff *skb)
{
    ......
    type = skb-&gt;protocol;
    ......
    ret = pt_prev-&gt;func(skb, skb-&gt;dev, pt_prev, orig_dev);
}
.......
</code></pre>
<h4 id="1-ip_rcv">1 ip_rcv</h4>
<ul>
<li>通过<strong>inet_protos 数组</strong>找到 tcp_v4_rcv 或者 udp_rcv 函数</li>
<li>执行<strong>NetFilter Iptables</strong>包过滤</li>
</ul>
<pre><code>
net/ipv4/ip_output.c

/*
 *  Main IP Receive routine.
 */
int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev)
{
    const struct iphdr *iph;
    u32 len;

    /* When the interface is in promisc. mode, drop all the crap
     * that it receives, do not try to analyse it.
     */
    if (skb-&gt;pkt_type == PACKET_OTHERHOST)
        goto drop;


    IP_UPD_PO_STATS_BH(dev_net(dev), IPSTATS_MIB_IN, skb-&gt;len);

    if ((skb = skb_share_check(skb, GFP_ATOMIC)) == NULL) {
        IP_INC_STATS_BH(dev_net(dev), IPSTATS_MIB_INDISCARDS);
        goto out;
    }

    if (!pskb_may_pull(skb, sizeof(struct iphdr)))
        goto inhdr_error;

    iph = ip_hdr(skb);

    /*
     *  RFC1122: 3.2.1.2 MUST silently discard any IP frame that fails the checksum.
     *
     *  Is the datagram acceptable?
     *
     *  1.  Length at least the size of an ip header
     *  2.  Version of 4
     *  3.  Checksums correctly. [Speed optimisation for later, skip loopback checksums]
     *  4.  Doesn't have a bogus length
     */

    if (iph-&gt;ihl &lt; 5 || iph-&gt;version != 4)
        goto inhdr_error;

    BUILD_BUG_ON(IPSTATS_MIB_ECT1PKTS != IPSTATS_MIB_NOECTPKTS + INET_ECN_ECT_1);
    BUILD_BUG_ON(IPSTATS_MIB_ECT0PKTS != IPSTATS_MIB_NOECTPKTS + INET_ECN_ECT_0);
    BUILD_BUG_ON(IPSTATS_MIB_CEPKTS != IPSTATS_MIB_NOECTPKTS + INET_ECN_CE);
    IP_ADD_STATS_BH(dev_net(dev),
            IPSTATS_MIB_NOECTPKTS + (iph-&gt;tos &amp; INET_ECN_MASK),
            max_t(unsigned short, 1, skb_shinfo(skb)-&gt;gso_segs));

    if (!pskb_may_pull(skb, iph-&gt;ihl*4))
        goto inhdr_error;

    iph = ip_hdr(skb);

    if (unlikely(ip_fast_csum((u8 *)iph, iph-&gt;ihl)))
        goto csum_error;

    len = ntohs(iph-&gt;tot_len);
    if (skb-&gt;len &lt; len) {
        IP_INC_STATS_BH(dev_net(dev), IPSTATS_MIB_INTRUNCATEDPKTS);
        goto drop;
    } else if (len &lt; (iph-&gt;ihl*4))
        goto inhdr_error;

    /* Our transport medium may have padded the buffer out. Now we know it
     * is IP we can trim to the true length of the frame.
     * Note this now means skb-&gt;len holds ntohs(iph-&gt;tot_len).
     */
    if (pskb_trim_rcsum(skb, len)) {
        IP_INC_STATS_BH(dev_net(dev), IPSTATS_MIB_INDISCARDS);
        goto drop;
    }

    skb-&gt;transport_header = skb-&gt;network_header + iph-&gt;ihl*4;

    /* Remove any debris in the socket control block */
    memset(IPCB(skb), 0, sizeof(struct inet_skb_parm));

    /* Must drop socket now because of tproxy. */
    skb_orphan(skb);

    return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL,
               ip_rcv_finish);

csum_error:
    IP_INC_STATS_BH(dev_net(dev), IPSTATS_MIB_CSUMERRORS);
inhdr_error:
    IP_INC_STATS_BH(dev_net(dev), IPSTATS_MIB_INHDRERRORS);
drop:
    kfree_skb(skb);
out:
    return NET_RX_DROP;
}


</code></pre>
<h4 id="2-tcp_v4_rcv">2 tcp_v4_rcv</h4>
<h4 id="4-udp_rcv">4 udp_rcv</h4>
<h4 id="5-napi_schedule">5 napi_schedule 系列函数</h4>
<h5 id="51-napi_schedule">5.1 <code>napi_schedule</code></h5>
<pre><code>
include/linux/netdevice.h:416:static inline void napi_schedule(struct napi_struct *n)


/**
 *  napi_schedule - schedule NAPI poll
 *  @n: napi context
 *
 * Schedule NAPI poll routine to be called if it is not already
 * running.
 */
static inline void napi_schedule(struct napi_struct *n)
{
    if (napi_schedule_prep(n))
        __napi_schedule(n);
}


</code></pre>
<h5 id="52-__napi_schedule">5.2 <code>__napi_schedule</code></h5>
<pre><code>
include/linux/netdevice.h

/**
 * __napi_schedule - schedule for receive
 * @n: entry to schedule
 *
 * The entry's receive function will be scheduled to run
 */
void __napi_schedule(struct napi_struct *n)
{
    unsigned long flags;

    local_irq_save(flags);
    ____napi_schedule(&amp;__get_cpu_var(softnet_data), n);
    local_irq_restore(flags);
}
EXPORT_SYMBOL(__napi_schedule);


</code></pre>
<h5 id="53-____napi_schedule">5.3 <code>____napi_schedule</code></h5>
<pre><code>
net/core/dev.c:3009:static inline void ____napi_schedule


/* Called with irq disabled */
static inline void ____napi_schedule(struct softnet_data *sd,
                     struct napi_struct *napi)
{
    list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);
    __raise_softirq_irqoff(NET_RX_SOFTIRQ);
}


</code></pre>
<h4 id="6-dev_hard_start_xmit">6 dev_hard_start_xmit 真正提交给硬件网卡发包函数</h4>
<pre><code>
net/core/dev.c

int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
            struct netdev_queue *txq)
{
    const struct net_device_ops *ops = dev-&gt;netdev_ops;
    int rc = NETDEV_TX_OK;
    unsigned int skb_len;

    if (likely(!skb-&gt;next)) {
        netdev_features_t features;

        /*
         * If device doesn't need skb-&gt;dst, release it right now while
         * its hot in this cpu cache
         */
        if (dev-&gt;priv_flags &amp; IFF_XMIT_DST_RELEASE)
            skb_dst_drop(skb);

        features = netif_skb_features(skb);

        if (vlan_tx_tag_present(skb) &amp;&amp;
            !vlan_hw_offload_capable(features, skb-&gt;vlan_proto)) {
            skb = __vlan_put_tag(skb, skb-&gt;vlan_proto,
                         vlan_tx_tag_get(skb));
            if (unlikely(!skb))
                goto out;

            skb-&gt;vlan_tci = 0;
        }

        /* If encapsulation offload request, verify we are testing
         * hardware encapsulation features instead of standard
         * features for the netdev
         */
        if (skb-&gt;encapsulation)
            features &amp;= dev-&gt;hw_enc_features;

        if (netif_needs_gso(skb, features)) {
            if (unlikely(dev_gso_segment(skb, features)))
                goto out_kfree_skb;
            if (skb-&gt;next)
                goto gso;
        } else {
            if (skb_needs_linearize(skb, features) &amp;&amp;
                __skb_linearize(skb))
                goto out_kfree_skb;

            /* If packet is not checksummed and device does not
             * support checksumming for this protocol, complete
             * checksumming here.
             */
            if (skb-&gt;ip_summed == CHECKSUM_PARTIAL) {
                if (skb-&gt;encapsulation)
                    skb_set_inner_transport_header(skb,
                        skb_checksum_start_offset(skb));
                else
                    skb_set_transport_header(skb,
                        skb_checksum_start_offset(skb));
                if (!(features &amp; NETIF_F_ALL_CSUM) &amp;&amp;
                     skb_checksum_help(skb))
                    goto out_kfree_skb;
            }
        }

        if (!list_empty(&amp;ptype_all))
            dev_queue_xmit_nit(skb, dev);

        skb_len = skb-&gt;len;
        trace_net_dev_start_xmit(skb, dev);
        rc = ops-&gt;ndo_start_xmit(skb, dev);
        trace_net_dev_xmit(skb, rc, dev, skb_len);
        if (rc == NETDEV_TX_OK)
            txq_trans_update(txq);
        return rc;
    }

gso:
    do {
        struct sk_buff *nskb = skb-&gt;next;

        skb-&gt;next = nskb-&gt;next;
        nskb-&gt;next = NULL;

        if (!list_empty(&amp;ptype_all))
            dev_queue_xmit_nit(nskb, dev);

        skb_len = nskb-&gt;len;
        trace_net_dev_start_xmit(nskb, dev);
        rc = ops-&gt;ndo_start_xmit(nskb, dev);
        trace_net_dev_xmit(nskb, rc, dev, skb_len);
        if (unlikely(rc != NETDEV_TX_OK)) {
            if (rc &amp; ~NETDEV_TX_MASK)
                goto out_kfree_gso_skb;
            nskb-&gt;next = skb-&gt;next;
            skb-&gt;next = nskb;
            return rc;
        }
        txq_trans_update(txq);
        if (unlikely(netif_xmit_stopped(txq) &amp;&amp; skb-&gt;next))
            return NETDEV_TX_BUSY;
    } while (skb-&gt;next);

out_kfree_gso_skb:
    if (likely(skb-&gt;next == NULL)) {
        skb-&gt;destructor = DEV_GSO_CB(skb)-&gt;destructor;
        consume_skb(skb);
        return rc;
    }
out_kfree_skb:
    kfree_skb(skb);
out:
    return rc;
}
EXPORT_SYMBOL_GPL(dev_hard_start_xmit);


</code></pre>
<h4 id="7-tcp_transmit_skb">7 tcp_transmit_skb</h4>
<ul>
<li>是发送数据位于传输层的最后一步</li>
<li>并且在 tcp_connect 函数中一开始发送 SYN 报文时也用到该函数.</li>
</ul>
<pre><code>

//file: net/ipv4/tcp_output.c
static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
    gfp_t gfp_mask)
{
 //1.克隆新 skb 出来
 if (likely(clone_it)) {
  skb = skb_clone(skb, gfp_mask);
  ......
 }

 //2.封装 TCP 头
 th = tcp_hdr(skb);
 th-&gt;source  = inet-&gt;inet_sport;
 th-&gt;dest  = inet-&gt;inet_dport;
 th-&gt;window  = ...;
 th-&gt;urg   = ...;
 ......

 //3.调用网络层发送接口
 err = icsk-&gt;icsk_af_ops-&gt;queue_xmit(skb, &amp;inet-&gt;cork.fl);
}

</code></pre>
<h2 id="_6">二 发包路径</h2>
<h3 id="1">1 架构图和总体纲领</h3>
<p><img alt="png" src="../../img/WechatIMG394.jpg" /></p>
<p><img alt="png" src="../../img/EEB8273F-9202-45B2-9859-982277C617DA.jpg" /></p>
<h3 id="2">2 硬中断代码</h3>
<ul>
<li>这里和<strong>收包路径</strong>的硬中断代码相同</li>
</ul>
<h3 id="3">3 内核事先做了哪些准备工作</h3>
<h3 id="4">4 具体路径</h3>
<h4 id="41">4.1 协议栈</h4>
<h4 id="42">4.2 邻居子系统</h4>
<ul>
<li>
<p><img alt="png" src="../../img/WechatIMG421.jpg" /></p>
</li>
<li>
<p><img alt="png" src="../../img/WechatIMG422.jpg" /></p>
</li>
<li>
<p><strong>邻居子系统</strong>嵌入在<strong>网络层</strong>和<strong>数据链路层</strong>之间（，可以对上和对下提供一个统一的接口，让两层间的通信变的透明。当然这里面有很大一部分原因得归功于地址解析协议（ARP）</p>
</li>
<li>L3 的是逻辑地址，L2 的是物理地址，需要做的就是实现这两个地址的映射。在 Linux 内核中把它单独做成子系统原因是网络层不止有 ipv4，还有其他协议，如果为每个 L3 协议做单独开发会有一些重复劳动，因此提炼一些通用数据结构减少这些重复劳动.</li>
</ul>
<h4 id="43">4.3 网络设备子系统</h4>
<ul>
<li>两大核心函数: <code>dev_queue_xmit</code>和<code>dev_hard_start_xmit</code></li>
<li>两大核心作用:</li>
<li>
<ul>
<li>查找一个合适网卡队列进行发送(因为现在网卡都是多队列)</li>
</ul>
</li>
<li>
<ul>
<li>执行 tc（流量控制）</li>
</ul>
</li>
</ul>
<p><img alt="png" src="../../img/WechatIMG419.jpg" /></p>
<p><img alt="png" src="../../img/WechatIMG420.jpg" /></p>
<h4 id="44">4.4 具体驱动程序</h4>
<h2 id="_7">三 收包路径</h2>
<h3 id="1_1">1 架构图和总体纲领</h3>
<p><img alt="png" src="../../img/3C3D69A4-9768-497D-96F2-A54901F4758F.png" /></p>
<p><img alt="png" src="../../img/WechatIMG386.jpg" /></p>
<p><img alt="png" src="../../img/5AA51F51-4101-4402-9B33-F442663B368A.png" /></p>
<h3 id="2_1">2 硬中断代码</h3>
<blockquote>
<p>以 igb 网卡为例</p>
</blockquote>
<pre><code>
//file: drivers/net/ethernet/intel/igb/igb_main.c

static irqreturn_t igb_msix_ring(int irq, void \*data){

    struct igb_q_vector *q_vector = data;

    /* Write the ITR value calculated from the previous interrupt. */
    igb_write_itr(q_vector);

    napi_schedule(&amp;q_vector-&gt;napi);
    return IRQ_HANDLED;

}

</code></pre>
<h4 id="21">2.1 网卡硬中断流程</h4>
<table>
<thead>
<tr>
<th>步骤</th>
<th>函数</th>
<th>是否开启硬中断</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>do_IRQ()</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>handle_IRQ_event()</td>
<td>3</td>
</tr>
<tr>
<td>3</td>
<td>驱动程序提供的 rtc_interrupt</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>do_softirq</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>net_rx_action</td>
<td>3</td>
</tr>
</tbody>
</table>
<h3 id="3_1">3 为了实现软中断内核做了哪些准备工作</h3>
<h4 id="31-ksoftirqd">3.1 开启 ksoftirqd 线程</h4>
<ul>
<li>为什么要创建这个线程？</li>
<li>
<ul>
<li>因为要利用这个线程去跑软中断. 每个软中断有对应的处理函数</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>软中断号</th>
<th>处理函数</th>
<th>注意点</th>
</tr>
</thead>
<tbody>
<tr>
<td>NET_TX_SOFTIRQ</td>
<td>net_tx_action</td>
<td></td>
</tr>
<tr>
<td>NET_RX_SOFTIRQ</td>
<td>net_rx_action</td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="32">3.2 统一实现、统一注册协议处理函数</h4>
<h5 id="321">3.2.1 实现</h5>
<table>
<thead>
<tr>
<th>协议</th>
<th>（接收方向）处理函数</th>
<th>注意点</th>
</tr>
</thead>
<tbody>
<tr>
<td>ip</td>
<td>ip_rcv</td>
<td></td>
</tr>
<tr>
<td>tcp</td>
<td>tcp_v4_rcv</td>
<td></td>
</tr>
<tr>
<td>udp</td>
<td>udp_rcv</td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="321_1">3.2.1 注册</h5>
<ul>
<li><strong>inet_protos 数组</strong>记录着 udp，tcp 的处理函数地址，<strong>ptype_base 哈希表</strong>存储着<code>ip_rcv()</code>函数的处理地址。</li>
</ul>
<p>内核实现了网络层的 ip 协议，也实现了传输层的 tcp 协议和 udp 协议。这些协议对应的实现函数分别是 ip_rcv(),tcp_v4_rcv()和 udp_rcv()。和我们平时写代码的方式不一样的是，内核是通过注册的方式来实现的。Linux 内核中的 fs_initcall 和 subsys_initcall 类似，也是初始化模块的入口。fs_initcall 调用 inet_init 后开始网络协议栈注册。通过 inet_init，将这些函数注册到了 inet_protos 和 ptype_base 数据结构中了。如下图:</p>
<p><img alt="png" src="../../img/WechatIMG389.jpg" /></p>
<p>相关代码如下</p>
<pre><code>
//file: net/ipv4/af_inet.c

static struct packet_type ip_packet_type __read_mostly = {

    .type = cpu_to_be16(ETH_P_IP),
    .func = ip_rcv

};

static const struct net_protocol udp_protocol = {
.handler = udp_rcv,
.err_handler = udp_err,
.no_policy = 1,
.netns_ok = 1

};

static const struct net_protocol tcp_protocol = {
.early_demux = tcp_v4_early_demux,
.handler = tcp_v4_rcv,
.err_handler = tcp_v4_err,
.no_policy = 1,
.netns_ok = 1,

};

static int __init inet_init(void){

    ......
    if (inet_add_protocol(&amp;icmp_protocol, IPPROTO_ICMP) &lt; 0)
        pr_crit(&quot;%s: Cannot add ICMP protocol\n&quot;, __func__);
    if (inet_add_protocol(&amp;udp_protocol, IPPROTO_UDP) &lt; 0)
        pr_crit(&quot;%s: Cannot add UDP protocol\n&quot;, __func__);
    if (inet_add_protocol(&amp;tcp_protocol, IPPROTO_TCP) &lt; 0)
        pr_crit(&quot;%s: Cannot add TCP protocol\n&quot;, __func__);
    ......
    dev_add_pack(&amp;ip_packet_type);

}

</code></pre>
<p>上面的代码中我们可以看到，<code>udp_protocol</code>结构体中的<code>handler</code>是<code>udp_rcv</code>，<code>tcp_protocol</code>结构体中的<code>handler</code>是<code>tcp_v4_rcv</code>，通过<code>inet_add_protocol</code>被初始化了进来。</p>
<pre><code>
int inet_add_protocol(const struct net_protocol \*prot, unsigned char protocol){
if (!prot-&gt;netns_ok) {
pr_err(&quot;Protocol %u is not namespace aware, cannot register.\n&quot;,
protocol);
return -EINVAL;
}

    return !cmpxchg((const struct net_protocol **)&amp;inet_protos[protocol],
            NULL, prot) ? 0 : -1;

}

</code></pre>
<p>inet_add_protocol 函数将 tcp 和 udp 对应的处理函数都注册到了 inet_protos 数组中了。再看<code>dev_add_pack(&amp;ip_packet_type);</code>这一行，ip_packet_type 结构体中的 type 是协议名，func 是 ip_rcv 函数，在 dev_add_pack 中会被注册到 ptype_base 哈希表中。</p>
<pre><code>
//file: net/core/dev.c

void dev_add_pack(struct packet_type \*pt){

    struct list_head *head = ptype_head(pt);
    ......

}

static inline struct list_head *ptype_head(const struct packet_type *pt){

    if (pt-&gt;type == htons(ETH_P_ALL))
        return &amp;ptype_all;
    else
        return &amp;ptype_base[ntohs(pt-&gt;type) &amp; PTYPE_HASH_MASK];

}

</code></pre>
<p>这里我们需要记住<strong>inet_protos 记录着 udp，tcp 的处理函数地址，ptype_base 存储着 ip_rcv()函数的处理地址</strong>。后面我们会看到<strong>软中断中会通过 ptype_base 找到 ip_rcv 函数地址，进而将 ip 包正确地送到 ip_rcv()中执行</strong>。<strong>在 ip_rcv 中将会通过 inet_protos 找到 tcp 或者 udp 的处理函数，再而把包转发给 udp_rcv()或 tcp_v4_rcv()函数</strong>。</p>
<p>扩展一下，如果看一下 ip_rcv 和 udp_rcv 等函数的代码能看到很多协议的处理过程。例如，<strong>ip_rcv 中会处理 netfilter 和 iptable 过滤，如果你有很多或者很复杂的 netfilter 或 iptables 规则，这些规则都是在软中断的上下文中执行的，会加大网络延迟。</strong>再例如，udp_rcv 中会判断 socket 接收队列是否满了。对应的相关内核参数是 net.core.rmem_max 和 net.core.rmem_default。如果有兴趣，建议大家好好读一下 inet_init 这个函数的代码。</p>
<h3 id="4_1">4 具体路径</h3>
<h4 id="40">4.0 简化版本:</h4>
<ul>
<li>硬中断(igb_msix_ring) ---&gt; napi_schedule()函数 ---&gt; 触发软中断(<code>__raise_softirq_irqoff</code>)</li>
<li>软中断(net_rx_action) ---&gt; 特定于网卡的 igb_poll()函数 ---&gt; napi_gro_receive ---&gt; netif_receive_skb ---&gt; ip_rcv</li>
</ul>
<h4 id="41_1">4.1 硬中断之前和硬中断的工作</h4>
<p>首先当数据帧从网线到达网卡上的时候，第一站是网卡的接收队列。网卡在分配给自己的 RingBuffer 中寻找可用的内存位置，找到后 DMA 引擎会把数据 DMA 到网卡之前关联的内存里，<strong>这个时候 CPU 都是无感的。</strong>当 DMA 操作完成以后，<strong>网卡会向 CPU 发起一个硬中断</strong>，通知 CPU 有数据到达。</p>
<p><img alt="png" src="../../img/WechatIMG386.jpg" /></p>
<blockquote>
<p>注意：当 RingBuffer 满的时候，新来的数据包将给丢弃。ifconfig 查看网卡的时候，可以里面有个 overruns，表示因为环形队列满被丢弃的包。如果发现有丢包，可能需要通过 ethtool 命令来加大环形队列的长度。</p>
</blockquote>
<p>在启动网卡一节，我们说到了网卡的硬中断注册的处理函数是 igb_msix_ring。</p>
<pre><code>
//file: drivers/net/ethernet/intel/igb/igb_main.c

static irqreturn_t igb_msix_ring(int irq, void \*data){

    struct igb_q_vector *q_vector = data;

    /* Write the ITR value calculated from the previous interrupt. */
    igb_write_itr(q_vector);

    napi_schedule(&amp;q_vector-&gt;napi);
    return IRQ_HANDLED;

}

</code></pre>
<p><code>igb_write_itr</code>只是记录一下硬件中断频率（据说目的是在减少对 CPU 的中断频率时用到）。顺着 napi_schedule 调用一路跟踪下去，</p>
<p><code>__napi_schedule=&gt;____napi_schedule</code></p>
<pre><code>
/* Called with irq disabled */

static inline void ____napi_schedule(struct softnet_data \*sd,

                     struct napi_struct *napi){
    list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);
    __raise_softirq_irqoff(NET_RX_SOFTIRQ);

}

</code></pre>
<p>这里我们看到，<code>list_add_tail</code>修改了 CPU 变量 softnet_data 里的 poll_list，将驱动 napi_struct 传过来的 poll_list 添加了进来。其中 softnet_data 中的 poll_list 是一个双向列表，其中的设备都带有输入帧等着被处理。紧接着<code>__raise_softirq_irqoff</code>触发了一个软中断 NET_RX_SOFTIRQ， 这个所谓的触发过程只是对一个变量进行了一次或运算而已。</p>
<pre><code>
void __raise_softirq_irqoff(unsigned int nr){
trace_softirq_raise(nr);
or_softirq_pending(1UL &lt;&lt; nr);

}

</code></pre>
<pre><code>
//file: include/linux/irq_cpustat.h

#define or_softirq_pending(x) (local_softirq_pending() |= (x))

</code></pre>
<p>我们说过，Linux 在硬中断里只完成简单必要的工作，剩下的大部分的处理都是转交给软中断的。通过上面代码可以看到，硬中断处理过程真的是非常短。只是记录了一个寄存器，修改了一下下 CPU 的 poll_list，然后发出个软中断。就这么简单，硬中断工作就算是完成了。</p>
<h4 id="42_1">4.2 软中断的工作</h4>
<p><img alt="png" src="../../img/WechatIMG387.jpg" /></p>
<h5 id="421-netif_receive_skb">4.2.1 进入协议栈之前的工作(netif_receive_skb 之前)</h5>
<p>内核线程初始化的时候，我们介绍了 ksoftirqd 中两个线程函数 ksoftirqd_should_run 和 run_ksoftirqd。其中 ksoftirqd_should_run 代码如下：</p>
<pre><code>
static int ksoftirqd_should_run(unsigned int cpu){
return local_softirq_pending();

}

#define local_softirq_pending() \ **IRQ_STAT(smp_processor_id(), **softirq_pending)

</code></pre>
<p>这里看到和硬中断中调用了同一个函数 local_softirq_pending。使用方式不同的是硬中断位置是为了写入标记，这里仅仅只是读取。如果硬中断中设置了 NET_RX_SOFTIRQ,这里自然能读取的到。接下来会真正进入线程函数中 run_ksoftirqd 处理：</p>
<pre><code>
static void run_ksoftirqd(unsigned int cpu){
local_irq_disable();
if (local_softirq_pending()) {
    __do_softirq();
    rcu_note_context_switch(cpu);
    local_irq_enable();
    cond_resched();
    return;
}
local_irq_enable();

}

</code></pre>
<p>在<code>__do_softirq</code>中，判断根据当前 CPU 的软中断类型，调用其注册的 action 方法。</p>
<pre><code>
asmlinkage void __do_softirq(void){
do {
    if (pending &amp; 1) {
        unsigned int vec_nr = h - softirq_vec;
        int prev_count = preempt_count();
        ...
        trace_softirq_entry(vec_nr);
        h-&gt;action(h);
        trace_softirq_exit(vec_nr);
        ...
    }
    h++;
    pending &gt;&gt;= 1;
} while (pending);

}

</code></pre>
<p>在网络子系统初始化小节， 我们看到我们为 NET_RX_SOFTIRQ 注册了处理函数 net_rx_action。所以 net_rx_action 函数就会被执行到了。</p>
<p>这里需要注意一个细节，硬中断中设置软中断标记，和 ksoftirq 的判断是否有软中断到达，都是基于 smp_processor_id()的。这意味着只要硬中断在哪个 CPU 上被响应，那么软中断也是在这个 CPU 上处理的。所以说，如果你发现你的 Linux 软中断 CPU 消耗都集中在一个核上的话，做法是要把调整硬中断的 CPU 亲和性，来将硬中断打散到不同的 CPU 核上去。</p>
<p>我们再来把精力集中到这个核心函数 net_rx_action 上来。</p>
<pre><code>
static void net_rx_action(struct softirq_action *h){
struct softnet_data *sd = &amp;__get_cpu_var(softnet_data);
unsigned long time_limit = jiffies + 2;
int budget = netdev_budget;
void \*have;

    local_irq_disable();
    while (!list_empty(&amp;sd-&gt;poll_list)) {
        ......
        n = list_first_entry(&amp;sd-&gt;poll_list, struct napi_struct, poll_list);

        work = 0;
        if (test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)) {
            work = n-&gt;poll(n, weight);
            trace_napi_poll(n);
        }
        budget -= work;
    }

}

</code></pre>
<p>函数开头的 time_limit 和 budget 是用来控制 net_rx_action 函数主动退出的，目的是保证网络包的接收不霸占 CPU 不放。等下次网卡再有硬中断过来的时候再处理剩下的接收数据包。其中 budget 可以通过内核参数调整。这个函数中剩下的核心逻辑是获取到当前 CPU 变量 softnet_data，对其 poll_list 进行遍历, 然后执行到网卡驱动注册到的 poll 函数。对于 igb 网卡来说，就是 igb 驱动力的 igb_poll 函数了。</p>
<pre><code>
static int igb_poll(struct napi_struct \*napi, int budget){
...
if (q_vector-&gt;tx.ring)
clean_complete = igb_clean_tx_irq(q_vector);

    if (q_vector-&gt;rx.ring)
        clean_complete &amp;= igb_clean_rx_irq(q_vector, budget);
    ...

}

</code></pre>
<p>在读取操作中，<code>igb_poll</code>的重点工作是对<code>igb_clean_rx_irq</code>的调用。</p>
<pre><code>
static bool igb_clean_rx_irq(struct igb_q_vector _q_vector, const int budget){
...
do {
/_ retrieve a buffer from the ring \*/
skb = igb_fetch_rx_buffer(rx_ring, rx_desc, skb);

        /* fetch next buffer in frame if non-eop */
        if (igb_is_non_eop(rx_ring, rx_desc))
            continue;
        }

        /* verify the packet layout is correct */
        if (igb_cleanup_headers(rx_ring, rx_desc, skb)) {
            skb = NULL;
            continue;
        }

        /* populate checksum, timestamp, VLAN, and protocol */
        igb_process_skb_fields(rx_ring, rx_desc, skb);

        napi_gro_receive(&amp;q_vector-&gt;napi, skb);

}

</code></pre>
<p>igb_fetch_rx_buffer 和 igb_is_non_eop 的作用就是把数据帧从 RingBuffer 上取下来。为什么需要两个函数呢？因为有可能帧要占多多个 RingBuffer，所以是在一个循环中获取的，直到帧尾部。获取下来的一个数据帧用一个 sk_buff 来表示。收取完数据以后，对其进行一些校验，然后开始设置 sbk 变量的 timestamp, VLAN id, protocol 等字段。接下来进入到 napi_gro_receive 中:</p>
<pre><code>
//file: net/core/dev.c

gro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb){

    skb_gro_reset_offset(skb);
    return napi_skb_finish(dev_gro_receive(napi, skb), skb);

}

</code></pre>
<p>dev_gro_receive 这个函数代表的是网卡 GRO 特性，可以简单理解成把相关的小包合并成一个大包就行，目的是减少传送给网络栈的包数，这有助于减少 CPU 的使用量。我们暂且忽略，直接看 napi_skb_finish, 这个函数主要就是调用了 netif_receive_skb。</p>
<pre><code>
//file: net/core/dev.c

static gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff \*skb){

    switch (ret) {
    case GRO_NORMAL:
        if (netif_receive_skb(skb))
            ret = GRO_DROP;
        break;
    ......

}

</code></pre>
<p>在<code>netif_receive_skb</code>中，数据包将被送到协议栈中。声明，以下的 3.3, 3.4, 3.5 也都属于软中断的处理过程，只不过由于篇幅太长，单独拿出来成小节。</p>
<h5 id="422-netif_receive_skb">4.2.2 进入协议栈之后的工作(netif_receive_skb 之后)</h5>
<ul>
<li><code>netif_receive_skb</code>函数会根据包的协议，假如是 udp 包，会将包依次送到<code>ip_rcv()</code>,<code>udp_rcv()</code>协议处理函数中进行处理。</li>
</ul>
<p><img alt="png" src="../../img/WechatIMG388.jpg" /></p>
<pre><code>
//file: net/core/dev.c

int netif_receive_skb(struct sk_buff \*skb){

    //RPS处理逻辑，先忽略    ......
    return __netif_receive_skb(skb);

}

static int \_\_netif_receive_skb(struct sk_buff \*skb){

    ......
    ret = __netif_receive_skb_core(skb, false);

}

static int \_\_netif_receive_skb_core(struct sk_buff \*skb, bool pfmemalloc){
......

    //pcap逻辑，这里会将数据送入抓包点。tcpdump就是从这个入口获取包的
    list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {
        if (!ptype-&gt;dev || ptype-&gt;dev == skb-&gt;dev) {
            if (pt_prev)
                ret = deliver_skb(skb, pt_prev, orig_dev);
            pt_prev = ptype;
        }
    }
    ......
    list_for_each_entry_rcu(ptype,
            &amp;ptype_base[ntohs(type) &amp; PTYPE_HASH_MASK], list) {
        if (ptype-&gt;type == type &amp;&amp;
            (ptype-&gt;dev == null_or_dev || ptype-&gt;dev == skb-&gt;dev ||
             ptype-&gt;dev == orig_dev)) {
            if (pt_prev)
                ret = deliver_skb(skb, pt_prev, orig_dev); // 在这里运行deliver_skb函数
            pt_prev = ptype;
        }
    }

}

</code></pre>
<p>在<code>__netif_receive_skb_core</code>中，我看着原来经常使用的 tcpdump 的抓包点，很是激动，看来读一遍源代码时间真的没白浪费。接着<code>__netif_receive_skb_core</code>取出 protocol，它会从数据包中取出协议信息，然后遍历注册在这个协议上的回调函数列表。<code>ptype_base</code> 是一个 <code>hash table</code>，在协议注册小节我们提到过。<code>ip_rcv</code> 函数地址就是存在这个 <code>hash table</code> 中的。</p>
<pre><code>
//file: net/core/dev.c

static inline int deliver_skb(struct sk_buff \*skb,

                  struct packet_type *pt_prev,
                  struct net_device *orig_dev){
    ......
    return pt_prev-&gt;func(skb, skb-&gt;dev, pt_prev, orig_dev);

}

</code></pre>
<ul>
<li><code>pt_prev-&gt;func</code>这一行就调用到了协议层注册的处理函数了。对于 ip 包来讲，就会进入到<code>ip_rcv</code>（如果是 arp 包的话，会进入到<code>arp_rcv</code>）。</li>
</ul>
<h2 id="_8">四 常见问题&amp;&amp;常用技能</h2>
<h3 id="1-cpu">1 调整网卡软中断到不同 CPU</h3>
<h4 id="_9">默认情况</h4>
<ul>
<li>当出行大流量下载的时候，网卡中断都在 cpu 0 上(cpu 0 的 system 时间非常高)</li>
</ul>
<p><img alt="png" src="../../img/WechatIMG369.jpg" /></p>
<ul>
<li>
<ul>
<li>通过<code>mpstat -I SUM -P ALL</code> 查看，所有的中断都在 cpu 0 上</li>
</ul>
</li>
</ul>
<p><img alt="png" src="../../img/WechatIMG370.jpg" /></p>
<ul>
<li>比如我要把网卡 0 号队列的中断绑定到 cpu0 和 cpu1 上，执行如下命令：</li>
</ul>
<pre><code>
echo 02 &gt; /proc/irq/149/smp_affinity

</code></pre>
<p>然后再进行施压：</p>
<p>发现 cpu1 已经分担了一部分压力，如下图所示：</p>
<p><img alt="png" src="../../img/WechatIMG371.jpg" /></p>
<ul>
<li>如何获取 0 号队列的中断号?</li>
</ul>
<pre><code>
grep eth0 /proc/interrupts

</code></pre>
<p><img alt="png" src="../../img/WechatIMG372.jpg" /></p>
<ul>
<li>可是为什么修改了硬中断到 CPU 的亲和性、却同时能够影响软中断的亲和性呢?</li>
</ul>
<p>硬中断中设置软中断标记和 ksoftirq 的判断是否有软中断到达，都使用到了<code>local_softirq_pending</code>函数, 都是基于<code>smp_processor_id()</code>的。这意味着<strong>只要硬中断在哪个 CPU 上被响应，那么软中断也是在这个 CPU 上处理的</strong>。所以说，如果你发现你的 Linux 软中断 CPU 消耗都集中在一个核上的话，做法是要把调整硬中断的 CPU 亲和性，来将硬中断打散到不同的 CPU 核上去。</p>
<ul>
<li>
<ul>
<li>硬中断 触发软中断的代码如下</li>
</ul>
</li>
</ul>
<pre><code>
void \_\_raise_softirq_irqoff(unsigned int nr){
trace_softirq_raise(nr);
or_softirq_pending(1UL &lt;&lt; nr);

}

//file: include/linux/irq_cpustat.h

#define or_softirq_pending(x) (local_softirq_pending() |= (x))

</code></pre>
<ul>
<li>
<ul>
<li>软中断<code>ksoftirqd_should_run</code>代码如下：</li>
</ul>
</li>
</ul>
<pre><code>
static int ksoftirqd_should_run(unsigned int cpu){
return local_softirq_pending();

}

#define local_softirq_pending() \ **IRQ_STAT(smp_processor_id(), **softirq_pending)

</code></pre>
<h3 id="2-netfilter-iptables">2 netfilter 和 iptables 过滤是在哪个环节/环境下执行的?</h3>
<ul>
<li>ip_rcv 中会处理 netfilter 和 iptable 过滤，如果你有很多或者很复杂的 netfilter 或 iptables 规则，这些规则都是在软中断的上下文中执行的，会加大网络延迟。</li>
</ul>
<h3 id="3-tcpdump">3 tcpdump 是在哪个环节执行的?</h3>
<h4 id="31">3.1 收包方向</h4>
<ul>
<li><code>ip_rcv —&gt; netfilter: raw_prerouting —&gt; conntrack —&gt; 路由(routing decision)</code></li>
<li>其中 tcpdump （如果）运行的话，是在 ip_rcv 之前 .</li>
</ul>
<p>tcpdump 是如何工作的?
tcpdump 工作在设备层，是通过虚协议的方式工作的。它通过调用 packet_create 将 抓 包 函 数 以 协 议 的 形 式 挂 到 ptype_all 上。</p>
<p>当收包的时候，驱动中实现的 igb_poll 函数最终会调用到<code>__nelif_receive_skb_core</code>, 这 个函数 会 在 将 包 送 到 协 议 栈 函数 ( ip_rcv、arp_rcv 等 ) 之 前 ， 将 包 先 送 到 <code>ptype_all</code> 抓 包 点。我们平时工作中经常会用到的 tcpdump 就是基于这些抓包点来工作的。</p>
<p>这 次 你 知 道 tcpdump 是 如 何 和 内 核 进 行 配 合 的 了吧 !</p>
<h4 id="32_1">3.2 发包方向</h4>
<h3 id="4-ebpf-xdp">4 eBPF 和 XDP 的包过滤移交到网卡上具体是啥?</h3>
<ul>
<li>说白了, 其实 XDP 是在网卡驱动层执行包过滤, 被过滤掉的包都不会分配 SKB.</li>
</ul>
<p><strong>XDP（eXpress Data Path）是基于 eBPF 实现的高性能、可编程的数据平面技术</strong>。基本的软件架构如下图所示</p>
<p><img alt="png" src="../../img/WechatIMG390.jpg" /></p>
<p><strong>XDP 位于网卡驱动层，当数据包经过 DMA 存放到 ring buffer 之后，分配 skb 之前，即可被 XDP 处理</strong> 。数据包经过 XDP 之后，会有 4 种去向：</p>
<ul>
<li>XDP_DROP：丢包</li>
<li>XDP_PASS：上送协议栈</li>
<li>XDP_TX：从当前网卡发送出去</li>
<li>XDP_REDIRECT：从其他网卡发送出去</li>
</ul>
<p>由于 <strong>XDP 位于整个 Linux 内核网络软件栈的底部，能够非常早地识别并丢弃攻击报文，具有很高的性能</strong>。这为我们改善 iptables/nftables 协议栈丢包的性能瓶颈，提供了非常棒的解决方案。</p>
<pre><code>
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../8_io_multiplexing/" class="btn btn-neutral float-left" title="第八篇 IO多路复用总结"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../11_igb/" class="btn btn-neutral float-right" title="第十一篇 igb网卡驱动梳理">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../8_io_multiplexing/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../11_igb/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
